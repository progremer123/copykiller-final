#!/usr/bin/env python3
"""ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""

import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime

def simple_crawl_test():
    print("ğŸŒ ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    
    # í¬ë¡¤ë§í•  ìƒ˜í”Œ ì‚¬ì´íŠ¸ë“¤ (í•œêµ­ì–´ ì½˜í…ì¸ )
    urls = [
        "https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5",
        "https://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5",
        "https://ko.wikipedia.org/wiki/%EA%B8%B0%ED%9B%84_%EB%B3%80%ED%99%94"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    articles = []
    
    for i, url in enumerate(urls, 1):
        try:
            print(f"ğŸ“„ í¬ë¡¤ë§ {i}/{len(urls)}: {url}")
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title_elem = soup.find('title')
                title = title_elem.text if title_elem else f"ì œëª©ì—†ìŒ_{i}"
                
                # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ë“¤)
                paragraphs = soup.find_all('p')
                content_parts = []
                
                for p in paragraphs[:10]:  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ë§Œ
                    text = p.get_text().strip()
                    if len(text) > 20:  # 20ì ì´ìƒë§Œ
                        content_parts.append(text)
                
                content = " ".join(content_parts)[:2000]  # 2000ìë¡œ ì œí•œ
                
                if len(content) > 100:
                    articles.append({
                        'title': title[:200],
                        'content': content,
                        'url': url,
                        'source_type': 'crawled'
                    })
                    print(f"âœ… ì„±ê³µ: {title[:50]}... ({len(content)}ì)")
                else:
                    print(f"âŒ ë‚´ìš© ë¶€ì¡±: {len(content)}ì")
            else:
                print(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    if articles:
        try:
            conn = sqlite3.connect("plagiarism.db")
            cursor = conn.cursor()
            
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
            saved_count = 0
            
            for article in articles:
                # ì¤‘ë³µ ì²´í¬
                cursor.execute(
                    "SELECT id FROM document_sources WHERE url = ? AND is_active = 1",
                    (article['url'],)
                )
                
                if cursor.fetchone():
                    print(f"âš ï¸  ì´ë¯¸ ì¡´ì¬: {article['title'][:30]}...")
                    continue
                
                cursor.execute("""
                    INSERT INTO document_sources 
                    (title, content, url, source_type, created_at, updated_at, is_active)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['title'],
                    article['content'],
                    article['url'],
                    article['source_type'],
                    current_time,
                    current_time,
                    1
                ))
                saved_count += 1
                print(f"ğŸ’¾ ì €ì¥: {article['title'][:30]}...")
            
            conn.commit()
            conn.close()
            
            print(f"\nğŸ‰ ì™„ë£Œ: {len(articles)}ê°œ í¬ë¡¤ë§, {saved_count}ê°œ ì €ì¥")
            
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
    else:
        print("âŒ í¬ë¡¤ë§ëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")

if __name__ == "__main__":
    simple_crawl_test()