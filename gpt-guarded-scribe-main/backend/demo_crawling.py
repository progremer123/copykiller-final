#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import time

def demonstrate_crawling():
    """ë‚˜ë¬´ìœ„í‚¤ì™€ ìœ„í‚¤ë°±ê³¼ í¬ë¡¤ë§ ê³¼ì • ì‹¤ì œ ì‹œì—°"""
    
    print("ğŸŒ ë‚˜ë¬´ìœ„í‚¤ì™€ ìœ„í‚¤ë°±ê³¼ í¬ë¡¤ë§ ê³¼ì • ì‹œì—°")
    print("=" * 60)
    
    # í¬ë¡¤ë§í•  URLë“¤
    urls = {
        "ìœ„í‚¤ë°±ê³¼": "https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5",
        "ë‚˜ë¬´ìœ„í‚¤": "https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5"
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for site_name, url in urls.items():
        print(f"\nğŸ“„ {site_name} í¬ë¡¤ë§ ì¤‘...")
        print(f"URL: {url}")
        
        try:
            # 1ë‹¨ê³„: HTTP ìš”ì²­
            print("1ï¸âƒ£ HTTP ìš”ì²­ ì¤‘...")
            response = requests.get(url, headers=headers, timeout=10)
            print(f"   ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            print(f"   ì‘ë‹µ í¬ê¸°: {len(response.text):,} ë¬¸ì")
            
            # 2ë‹¨ê³„: HTML íŒŒì‹±
            print("2ï¸âƒ£ HTML íŒŒì‹± ì¤‘...")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 3ë‹¨ê³„: ì œëª© ì¶”ì¶œ
            print("3ï¸âƒ£ ì œëª© ì¶”ì¶œ...")
            title_element = soup.find('title')
            if title_element:
                title = title_element.get_text().strip()
                print(f"   ì œëª©: {title[:60]}...")
            
            # 4ë‹¨ê³„: ë³¸ë¬¸ ì¶”ì¶œ
            print("4ï¸âƒ£ ë³¸ë¬¸ ì¶”ì¶œ...")
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            content = ""
            if site_name == "ìœ„í‚¤ë°±ê³¼":
                # ìœ„í‚¤ë°±ê³¼ íŠ¹í™” ì¶”ì¶œ
                content_div = soup.find('div', {'class': 'mw-parser-output'})
                if content_div:
                    paragraphs = content_div.find_all('p')
                    content = ' '.join([p.get_text() for p in paragraphs[:5]])  # ì²« 5ê°œ ë¬¸ë‹¨
            
            elif site_name == "ë‚˜ë¬´ìœ„í‚¤":
                # ë‚˜ë¬´ìœ„í‚¤ íŠ¹í™” ì¶”ì¶œ
                paragraphs = soup.find_all('p')
                if paragraphs:
                    content = ' '.join([p.get_text() for p in paragraphs[:3]])  # ì²« 3ê°œ ë¬¸ë‹¨
            
            if not content:
                # í´ë°±: ëª¨ë“  p íƒœê·¸
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text() for p in paragraphs[:3]])
            
            # 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ
            print("5ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ...")
            import re
            content = re.sub(r'\s+', ' ', content)  # ê³µë°± ì •ë¦¬
            content = content.strip()[:500]  # ê¸¸ì´ ì œí•œ
            
            print(f"   ì¶”ì¶œëœ ë‚´ìš© ({len(content)}ì): {content}...")
            
            # 6ë‹¨ê³„: í‚¤ì›Œë“œ ë¶„ì„
            print("6ï¸âƒ£ í‚¤ì›Œë“œ ë¶„ì„...")
            words = content.split()
            korean_words = [word for word in words if any('\uAC00' <= char <= '\uD7A3' for char in word)]
            print(f"   í•œê¸€ í‚¤ì›Œë“œ ìƒ˜í”Œ: {korean_words[:10]}")
            
        except Exception as e:
            print(f"âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        print("-" * 60)
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    print("\nâœ… í¬ë¡¤ë§ ê³¼ì • ì™„ë£Œ!")

if __name__ == "__main__":
    demonstrate_crawling()