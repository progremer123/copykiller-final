from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Request
from sqlalchemy.orm import Session
from typing import List, Optional, Dict
import uuid
import time
from datetime import datetime

from database import get_db
from models import PlagiarismCheck, PlagiarismMatch
from services.plagiarism_service import PlagiarismService
from services.text_processor import TextProcessor
from services.web_crawler_service import WebCrawlerService
from services.ai_crawler_service import AICrawlerService
from services.ai_knowledge_generator import AIKnowledgeGenerator
from services.ai_plagiarism_avoidance import AIPlagiarismAvoidance
from services.ai_plagiarism_fixer import AIPlagiarismFixer
from services.sentence_improvement_service import SentenceImprovementService
from schemas import PlagiarismCheckCreate, PlagiarismCheckResponse, PlagiarismMatchResponse
import sqlite3
# â¬‡ï¸â¬‡ï¸â¬‡ï¸ 1. Celery ì‘ì—…ì„ ì§ì ‘ import í•©ë‹ˆë‹¤. â¬‡ï¸â¬‡ï¸â¬‡ï¸
# ì„ì‹œë¡œ Celery ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬
# from tasks.plagiarism_tasks import process_plagiarism_check

router = APIRouter()

@router.get("/health")
async def api_health_check():
    """API ìƒíƒœ í™•ì¸"""
    return {"status": "healthy", "service": "plagiarism API"}

@router.get("/database/stats")
async def get_database_stats(db: Session = Depends(get_db)):
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ í™•ì¸"""
    service = PlagiarismService(db)
    stats = service.get_database_stats()
    return stats

@router.post("/check/text", response_model=PlagiarismCheckResponse)
async def check_text_plagiarism(
    payload: PlagiarismCheckCreate,
    db: Session = Depends(get_db)
):
    """í…ìŠ¤íŠ¸ í‘œì ˆ ê²€ì‚¬"""
    print(f"[*] í‘œì ˆ ê²€ì‚¬ ìš”ì²­ ë°›ìŒ: {len(payload.text)}ì")
    try:
        text = payload.text
        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 10ì)")
        
        check_id = str(uuid.uuid4())
        print(f"[*] ê²€ì‚¬ ID ìƒì„±: {check_id}")
        service = PlagiarismService(db)
        
        check = service.create_check(check_id, text)
        
        # ì¦‰ì‹œ ë™ê¸° ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
        try:
            print(f"[*] í‘œì ˆ ê²€ì‚¬ ì²˜ë¦¬ ì‹œì‘...")
            service.process_plagiarism_check(check_id, text)
            print(f"[OK] í‘œì ˆ ê²€ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
            # ì²˜ë¦¬ ì™„ë£Œ í›„ ê²°ê³¼ ì¬ì¡°íšŒ
            updated_check = service.get_check_result(check_id)
            if updated_check:
                check = updated_check
                print(f"[*] ì—…ë°ì´íŠ¸ëœ ê²°ê³¼ ì¡°íšŒ ì™„ë£Œ")
        except Exception as e:
            print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        
        # ë§¤ì¹˜ ì •ë³´ í¬í•¨
        matches = []
        if hasattr(check, 'matches') and check.matches:
            matches = [
                PlagiarismMatchResponse(
                    matched_text=match.matched_text or "",
                    source_title=match.source_title or "Unknown",
                    source_url=match.source_url or "",
                    similarity_score=match.similarity_score or 0.0,
                    start_index=match.start_index or 0,
                    end_index=match.end_index or 0
                )
                for match in check.matches
            ]
        
        # ì‘ë‹µ ë°˜í™˜ (Pydantic ëª¨ë¸ì´ ìë™ìœ¼ë¡œ JSON ë³€í™˜)
        return PlagiarismCheckResponse(
            id=check.id,
            original_text=check.original_text,
            similarity_score=check.similarity_score or 0.0,
            status=check.status,
            created_at=check.created_at,
            matches=matches
        )
    except Exception as e:
        print(f"API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")

@router.post("/check/file", response_model=PlagiarismCheckResponse)
async def check_file_plagiarism(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """íŒŒì¼ í‘œì ˆ ê²€ì‚¬"""
    allowed_types = ["text/plain", "application/pdf", "application/msword", 
                    "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]
    
    if file.content_type not in allowed_types:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…ì…ë‹ˆë‹¤")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (ìµœëŒ€ 10MB)")
    
    processor = TextProcessor()
    text = processor.extract_text_from_file(content, file.content_type)
    
    if not text or len(text.strip()) < 10:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    check_id = str(uuid.uuid4())
    service = PlagiarismService(db)
    
    check = service.create_check(
        check_id, 
        text, 
        file_name=file.filename,
        file_type=file.content_type
    )
    
    # ì¦‰ì‹œ ë™ê¸° ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
    try:
        service.process_plagiarism_check(check_id, text)
        # ì²˜ë¦¬ ì™„ë£Œ í›„ ê²°ê³¼ ì¬ì¡°íšŒ
        updated_check = service.get_check_result(check_id)
        if updated_check:
            check = updated_check
    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
    
    return PlagiarismCheckResponse(
        id=check.id,
        original_text=check.original_text,
        similarity_score=check.similarity_score,
        status=check.status,
        created_at=check.created_at,
        matches=[]
    )

# ... (ì´í•˜ ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼)
@router.get("/check/{check_id}", response_model=PlagiarismCheckResponse)
async def get_plagiarism_result(check_id: str, db: Session = Depends(get_db)):
    """í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ"""
    service = PlagiarismService(db)
    check = service.get_check_result(check_id)
    
    if not check:
        raise HTTPException(status_code=404, detail="ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    matches = [
        PlagiarismMatchResponse(
            matched_text=match.matched_text,
            source_title=match.source_title,
            source_url=match.source_url,
            similarity_score=match.similarity_score,
            start_index=match.start_index,
            end_index=match.end_index
        )
        for match in check.matches
    ]
    
    return PlagiarismCheckResponse(
        id=check.id,
        original_text=check.original_text,
        similarity_score=check.similarity_score,
        status=check.status,
        created_at=check.created_at,
        processing_time=check.processing_time,
        matches=matches
    )

@router.get("/history", response_model=List[PlagiarismCheckResponse])
async def get_check_history(
    limit: int = 10,
    offset: int = 0,
    db: Session = Depends(get_db)
):
    """ê²€ì‚¬ ì´ë ¥ ì¡°íšŒ"""
    service = PlagiarismService(db)
    checks = service.get_check_history(limit, offset)
    
    return [
        PlagiarismCheckResponse(
            id=check.id,
            original_text=check.original_text,
            similarity_score=check.similarity_score,
            status=check.status,
            created_at=check.created_at,
            processing_time=check.processing_time,
            matches=[]
        )
        for check in checks
    ]

@router.delete("/check/{check_id}")
async def delete_check(check_id: str, db: Session = Depends(get_db)):
    """ê²€ì‚¬ ê²°ê³¼ ì‚­ì œ"""
    service = PlagiarismService(db)
    success = service.delete_check(check_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return {"message": "ê²€ì‚¬ ê²°ê³¼ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}

@router.post("/crawl")
async def crawl_web_content(
    query: str,
    num_results: int = 5,
    db: Session = Depends(get_db)
):
    """ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ìƒˆë¡œìš´ ì½˜í…ì¸  ì¶”ê°€"""
    if not query or len(query.strip()) < 2:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ëŠ” 2ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    if num_results < 1 or num_results > 20:
        raise HTTPException(status_code=400, detail="ê²°ê³¼ ê°œìˆ˜ëŠ” 1~20 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    try:
        crawler = WebCrawlerService()
        result = crawler.crawl_and_save(query.strip(), num_results)
        
        return {
            "message": "ì›¹ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "query": result["query"],
            "total_crawled": result["total_crawled"],
            "saved_count": result["saved_count"],
            "articles": result["articles"]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

@router.get("/database/stats")
async def get_database_stats(db: Session = Depends(get_db)):
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
    try:
        from models import DocumentSource
        
        # ì´ ë¬¸ì„œ ìˆ˜
        total_docs = db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
        
        # ì†ŒìŠ¤ íƒ€ì…ë³„ í†µê³„
        from sqlalchemy import func
        type_stats = db.query(
            DocumentSource.source_type,
            func.count(DocumentSource.id).label('count')
        ).filter(
            DocumentSource.is_active == True
        ).group_by(DocumentSource.source_type).all()
        
        return {
            "total_documents": total_docs,
            "source_types": [
                {"type": stat.source_type, "count": stat.count}
                for stat in type_stats
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@router.post("/improve/text")
async def improve_text_suggestions(
    payload: dict,
    db: Session = Depends(get_db)
):
    """í…ìŠ¤íŠ¸ ë¬¸ì¥ ê°œì„  ì œì•ˆ"""
    try:
        text = payload.get("text", "")
        check_id = payload.get("check_id")  # ì˜µì…˜: í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ í™œìš©
        
        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 10ì)")
        
        improvement_service = SentenceImprovementService()
        
        # í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° í™œìš©
        plagiarism_matches = []
        if check_id:
            plagiarism_service = PlagiarismService(db)
            check_result = plagiarism_service.get_check_result(check_id)
            if check_result and hasattr(check_result, 'matches') and check_result.matches:
                plagiarism_matches = [
                    {
                        "matched_text": match.matched_text or "",
                        "similarity_score": match.similarity_score or 0.0,
                        "source_title": match.source_title or "",
                        "start_index": match.start_index or 0,
                        "end_index": match.end_index or 0
                    }
                    for match in check_result.matches
                ]
        
        # ë¬¸ì¥ ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = improvement_service.generate_improvement_suggestions(
            text, plagiarism_matches if plagiarism_matches else None
        )
        
        # API ì‘ë‹µ í˜•íƒœë¡œ í¬ë§·
        formatted_result = improvement_service.format_suggestions_for_api(suggestions)
        
        return {
            "success": True,
            "original_text": text,
            "improvement_data": formatted_result,
            "message": f"{formatted_result['total_suggestions']}ê°œì˜ ê°œì„  ì œì•ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"ë¬¸ì¥ ê°œì„  API ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë¬¸ì¥ ê°œì„  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.post("/improve/check/{check_id}")
async def improve_plagiarism_result(
    check_id: str,
    db: Session = Depends(get_db)
):
    """í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¬¸ì¥ ê°œì„  ì œì•ˆ"""
    try:
        # í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ
        plagiarism_service = PlagiarismService(db)
        check_result = plagiarism_service.get_check_result(check_id)
        
        if not check_result:
            raise HTTPException(status_code=404, detail="í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        original_text = check_result.original_text
        if not original_text or len(original_text.strip()) < 10:
            raise HTTPException(status_code=400, detail="ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
        
        # í‘œì ˆ ë§¤ì¹˜ ì •ë³´ ì¶”ì¶œ
        plagiarism_matches = []
        if hasattr(check_result, 'matches') and check_result.matches:
            plagiarism_matches = [
                {
                    "matched_text": match.matched_text or "",
                    "similarity_score": match.similarity_score or 0.0,
                    "source_title": match.source_title or "",
                    "start_index": match.start_index or 0,
                    "end_index": match.end_index or 0
                }
                for match in check_result.matches
            ]
        
        # ë¬¸ì¥ ê°œì„  ì œì•ˆ ìƒì„±
        improvement_service = SentenceImprovementService()
        suggestions = improvement_service.generate_improvement_suggestions(
            original_text, plagiarism_matches
        )
        
        # API ì‘ë‹µ í˜•íƒœë¡œ í¬ë§·
        formatted_result = improvement_service.format_suggestions_for_api(suggestions)
        
        return {
            "success": True,
            "check_id": check_id,
            "original_text": original_text,
            "similarity_score": check_result.similarity_score or 0.0,
            "plagiarism_matches_count": len(plagiarism_matches),
            "improvement_data": formatted_result,
            "message": f"í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ {formatted_result['total_suggestions']}ê°œì˜ ê°œì„  ì œì•ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"í‘œì ˆ ê²°ê³¼ ê¸°ë°˜ ê°œì„  API ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ê°œì„  ì œì•ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ==================== AI í¬ë¡¤ë§ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/crawl/ai-enhanced")
async def ai_enhanced_crawl(
    query: str,
    num_results: int = 15,
    db: Session = Depends(get_db)
):
    """AI ê¸°ë°˜ ê³ ê¸‰ ì›¹ í¬ë¡¤ë§"""
    print(f"ğŸ¤– AI í¬ë¡¤ë§ ìš”ì²­: '{query}' (ê²°ê³¼ ìˆ˜: {num_results})")
    
    try:
        if not query or len(query.strip()) < 2:
            raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 2ì)")
        
        # AI í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        ai_crawler = AICrawlerService()
        
        # AI ê°•í™” í¬ë¡¤ë§ ì‹¤í–‰
        result = ai_crawler.ai_enhanced_crawl(query.strip(), num_results)
        
        return {
            "success": True,
            "query": query,
            "crawling_result": result,
            "summary": {
                "total_collected": result['total_crawled'],
                "successfully_saved": result['saved_count'],
                "sources_used": result['sources_used'],
                "coverage_ratio": f"{(result['saved_count'] / max(result['total_crawled'], 1) * 100):.1f}%"
            },
            "message": f"'{query}' ê´€ë ¨ ì½˜í…ì¸  {result['saved_count']}ê°œë¥¼ {len(result['sources_used'])}ê°œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"AI í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/crawl/sources")
async def get_crawl_sources():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ì†ŒìŠ¤ ëª©ë¡"""
    ai_crawler = AICrawlerService()
    
    sources = [
        {
            "key": key,
            "name": target.name,
            "domain": target.domain,
            "description": f"{target.name}ì—ì„œ í•œêµ­ì–´ ì½˜í…ì¸  ìˆ˜ì§‘"
        }
        for key, target in ai_crawler.crawl_targets.items()
    ]
    
    return {
        "success": True,
        "total_sources": len(sources),
        "sources": sources,
        "capabilities": [
            "ì§€ëŠ¥í˜• ê²€ìƒ‰ì–´ í™•ì¥",
            "ë‹¤ì¤‘ ì†ŒìŠ¤ ë™ì‹œ í¬ë¡¤ë§",
            "ì¤‘ë³µ ì½˜í…ì¸  ìë™ í•„í„°ë§",
            "í’ˆì§ˆ ê¸°ë°˜ ì½˜í…ì¸  ì„ ë³„",
            "ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"
        ]
    }

@router.post("/crawl/batch")
async def batch_ai_crawl(
    queries: List[str],
    results_per_query: int = 10,
    db: Session = Depends(get_db)
):
    """ì—¬ëŸ¬ ì£¼ì œì— ëŒ€í•œ ë°°ì¹˜ AI í¬ë¡¤ë§"""
    print(f"ğŸ“¦ ë°°ì¹˜ AI í¬ë¡¤ë§ ìš”ì²­: {len(queries)}ê°œ ì£¼ì œ")
    
    if not queries or len(queries) == 0:
        raise HTTPException(status_code=400, detail="ìµœì†Œ 1ê°œ ì´ìƒì˜ ê²€ìƒ‰ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    if len(queries) > 10:
        raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    try:
        ai_crawler = AICrawlerService()
        batch_results = []
        total_collected = 0
        total_saved = 0
        
        for i, query in enumerate(queries):
            print(f"ğŸ”„ ì§„í–‰ë¥ : {i+1}/{len(queries)} - '{query}' ì²˜ë¦¬ ì¤‘...")
            
            try:
                result = ai_crawler.ai_enhanced_crawl(query.strip(), results_per_query)
                batch_results.append({
                    "query": query,
                    "status": "success",
                    "collected": result['total_crawled'],
                    "saved": result['saved_count'],
                    "sources": result['sources_used']
                })
                
                total_collected += result['total_crawled']
                total_saved += result['saved_count']
                
            except Exception as e:
                print(f"[ERROR] '{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                batch_results.append({
                    "query": query,
                    "status": "failed",
                    "error": str(e),
                    "collected": 0,
                    "saved": 0,
                    "sources": []
                })
        
        successful_queries = [r for r in batch_results if r["status"] == "success"]
        failed_queries = [r for r in batch_results if r["status"] == "failed"]
        
        return {
            "success": True,
            "batch_summary": {
                "total_queries": len(queries),
                "successful": len(successful_queries),
                "failed": len(failed_queries),
                "total_collected": total_collected,
                "total_saved": total_saved,
                "success_rate": f"{(len(successful_queries) / len(queries) * 100):.1f}%"
            },
            "results": batch_results,
            "message": f"{len(successful_queries)}/{len(queries)}ê°œ ì£¼ì œì—ì„œ ì´ {total_saved}ê°œ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"ë°°ì¹˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/crawl/stats")
async def get_crawl_statistics(db: Session = Depends(get_db)):
    """í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ"""
    try:
        service = PlagiarismService(db)
        stats = service.get_database_stats()
        
        # AI í¬ë¡¤ë§ ê´€ë ¨ í†µê³„ ì¶”ê°€
        ai_stats = {
            "ai_crawling_enabled": True,
            "supported_sources": 8,
            "languages": ["í•œêµ­ì–´"],
            "features": [
                "ì§€ëŠ¥í˜• ê²€ìƒ‰ì–´ í™•ì¥",
                "ë‹¤ì¤‘ ì†ŒìŠ¤ í¬ë¡¤ë§",
                "ìë™ ì¤‘ë³µ ì œê±°",
                "í’ˆì§ˆ í•„í„°ë§"
            ]
        }

        
        return {
            "success": True,
            "database_stats": stats,
            "ai_crawling_stats": ai_stats,
            "message": "í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ ì™„ë£Œ"
        }
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ==================== AI ì§€ì‹ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/ai-knowledge/generate")
async def generate_ai_knowledge(
    topic: str,
    num_articles: int = 5,
    db: Session = Depends(get_db)
):
    """Claude AIë¥¼ í™œìš©í•œ ì§€ì‹ ì½˜í…ì¸  ìƒì„±"""
    print(f"ğŸ¤– AI ì§€ì‹ ìƒì„± ìš”ì²­: '{topic}' (ìƒì„± ìˆ˜: {num_articles})")
    
    try:
        if not topic or len(topic.strip()) < 2:
            raise HTTPException(status_code=400, detail="ì£¼ì œê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 2ì)")
        
        if num_articles > 10:
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ìƒì„± ê°€ëŠ¥í•©ë‹ˆë‹¤")
        
        # AI ì§€ì‹ ìƒì„±ê¸° ì´ˆê¸°í™”
        ai_generator = AIKnowledgeGenerator()
        
        # AI ì§€ì‹ ìƒì„± ë° ì €ì¥
        result = ai_generator.generate_and_save_knowledge(topic.strip(), num_articles)
        
        return {
            "success": True,
            "topic": topic,
            "ai_generation_result": result,
            "summary": {
                "requested_articles": result['requested_count'],
                "generated_articles": result['generated_count'],
                "saved_articles": result['saved_count'],
                "generation_rate": f"{(result['generated_count'] / result['requested_count'] * 100):.1f}%",
                "save_rate": f"{(result['saved_count'] / max(result['generated_count'], 1) * 100):.1f}%"
            },
            "message": f"'{topic}' ì£¼ì œë¡œ AIê°€ {result['saved_count']}ê°œì˜ ì§€ì‹ ì½˜í…ì¸ ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"AI ì§€ì‹ ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI ì§€ì‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.post("/ai-knowledge/batch-generate")
async def batch_generate_ai_knowledge(
    topics: List[str],
    articles_per_topic: int = 3,
    db: Session = Depends(get_db)
):
    """ì—¬ëŸ¬ ì£¼ì œì— ëŒ€í•œ AI ì§€ì‹ ë°°ì¹˜ ìƒì„±"""
    print(f"ğŸ“¦ AI ì§€ì‹ ë°°ì¹˜ ìƒì„± ìš”ì²­: {len(topics)}ê°œ ì£¼ì œ")
    
    if not topics or len(topics) == 0:
        raise HTTPException(status_code=400, detail="ìµœì†Œ 1ê°œ ì´ìƒì˜ ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    if len(topics) > 5:
        raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 5ê°œ ì£¼ì œê¹Œì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    try:
        ai_generator = AIKnowledgeGenerator()
        batch_results = []
        total_generated = 0
        total_saved = 0
        
        for i, topic in enumerate(topics):
            print(f"ğŸ”„ AI ìƒì„± ì§„í–‰ë¥ : {i+1}/{len(topics)} - '{topic}' ì²˜ë¦¬ ì¤‘...")
            
            try:
                result = ai_generator.generate_and_save_knowledge(topic.strip(), articles_per_topic)
                batch_results.append({
                    "topic": topic,
                    "status": "success",
                    "generated": result['generated_count'],
                    "saved": result['saved_count'],
                    "contents": result['contents_summary']
                })
                
                total_generated += result['generated_count']
                total_saved += result['saved_count']
                
            except Exception as e:
                print(f"[ERROR] '{topic}' AI ìƒì„± ì‹¤íŒ¨: {e}")
                batch_results.append({
                    "topic": topic,
                    "status": "failed",
                    "error": str(e),
                    "generated": 0,
                    "saved": 0,
                    "contents": []
                })
        
        successful_topics = [r for r in batch_results if r["status"] == "success"]
        failed_topics = [r for r in batch_results if r["status"] == "failed"]
        
        return {
            "success": True,
            "batch_summary": {
                "total_topics": len(topics),
                "successful": len(successful_topics),
                "failed": len(failed_topics),
                "total_generated": total_generated,
                "total_saved": total_saved,
                "success_rate": f"{(len(successful_topics) / len(topics) * 100):.1f}%"
            },
            "results": batch_results,
            "message": f"{len(successful_topics)}/{len(topics)}ê°œ ì£¼ì œì—ì„œ ì´ {total_saved}ê°œì˜ AI ì§€ì‹ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"AI ì§€ì‹ ë°°ì¹˜ ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI ì§€ì‹ ë°°ì¹˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/ai-knowledge/capabilities")
async def get_ai_knowledge_capabilities():
    """AI ì§€ì‹ ìƒì„± ê¸°ëŠ¥ ì†Œê°œ"""
    ai_generator = AIKnowledgeGenerator()
    
    return {
        "success": True,
        "ai_generator_info": {
            "name": "Claude AI ì§€ì‹ ìƒì„±ê¸°",
            "description": "Claude AIë¥¼ í™œìš©í•˜ì—¬ ì£¼ì œë³„ ì „ë¬¸ ì§€ì‹ ì½˜í…ì¸ ë¥¼ ì‹¤ì‹œê°„ ìƒì„±",
            "supported_topics": list(ai_generator.knowledge_templates.keys()),
            "features": [
                "ì‹¤ì‹œê°„ AI ì½˜í…ì¸  ìƒì„±",
                "ì£¼ì œë³„ ì „ë¬¸ ì§€ì‹ êµ¬ì¡°í™”",
                "ë‹¤ì–‘í•œ ê´€ì ì˜ ë¶„ì„ ì œê³µ",
                "í•™ìˆ ì  ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼",
                "ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›",
                "ìë™ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"
            ],
            "advantages": [
                "ì›¹ í¬ë¡¤ë§ ì œí•œ ì—†ìŒ",
                "ì‹¤ì‹œê°„ ìµœì‹  ì§€ì‹ ë°˜ì˜",
                "ì¼ê´€ì„± ìˆëŠ” í’ˆì§ˆ",
                "ì €ì‘ê¶Œ ë¬¸ì œ ì—†ìŒ",
                "ë¬´ì œí•œ í™•ì¥ ê°€ëŠ¥"
            ]
        },
        "usage_examples": [
            {
                "topic": "ì¸ê³µì§€ëŠ¥",
                "generated_subtopics": ["AIì˜ ì •ì˜ì™€ ê°œë…", "AI ë°œì „ ì—­ì‚¬", "AI ê¸°ìˆ  ë¶„ë¥˜"]
            },
            {
                "topic": "ê¸°í›„ë³€í™”", 
                "generated_subtopics": ["ê¸°í›„ë³€í™”ì˜ ì›ì¸", "ì˜¨ì‹¤ê°€ìŠ¤ íš¨ê³¼", "ê¸°í›„ë³€í™” ì˜í–¥"]
            }
        ]
    }

@router.get("/ai-knowledge/stats")
async def get_ai_knowledge_stats(db: Session = Depends(get_db)):
    """AI ìƒì„± ì§€ì‹ í†µê³„"""
    try:
        service = PlagiarismService(db)
        all_stats = service.get_database_stats()
        
        # AI ìƒì„± ì½˜í…ì¸ ë§Œ í•„í„°ë§í•´ì„œ í†µê³„ ê³„ì‚°
        conn = sqlite3.connect("plagiarism.db")
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT COUNT(*) FROM document_sources 
            WHERE source_type LIKE '%ai_generated%' AND is_active = 1
        """)
        ai_generated_count = cursor.fetchone()[0]
        
        cursor.execute("""
            SELECT source_type, COUNT(*) FROM document_sources 
            WHERE source_type LIKE '%ai_generated%' AND is_active = 1
            GROUP BY source_type
        """)
        ai_types = cursor.fetchall()
        
        conn.close()
        
        ai_stats = {
            "total_ai_documents": ai_generated_count,
            "ai_document_types": dict(ai_types),
            "ai_generation_enabled": True,
            "supported_languages": ["í•œêµ­ì–´"],
            "generation_capabilities": [
                "ì‹¤ì‹œê°„ ì½˜í…ì¸  ìƒì„±",
                "ì£¼ì œë³„ ì „ë¬¸ ì§€ì‹",
                "êµ¬ì¡°í™”ëœ ë¬¸ì„œ",
                "ë°°ì¹˜ ì²˜ë¦¬"
            ]
        }
        
        return {
            "success": True,
            "overall_stats": all_stats,
            "ai_knowledge_stats": ai_stats,
            "message": f"ì „ì²´ ë¬¸ì„œ {all_stats.get('total_documents', 0)}ê°œ ì¤‘ AI ìƒì„± {ai_generated_count}ê°œ"
        }
        
    except Exception as e:
        print(f"AI ì§€ì‹ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"AI ì§€ì‹ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ==================== AI í‘œì ˆ íšŒí”¼ ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/avoid-plagiarism/{check_id}")
async def avoid_plagiarism_ai(
    check_id: str,
    db: Session = Depends(get_db)
):
    """AI ê¸°ë°˜ í‘œì ˆ íšŒí”¼ - í‘œì ˆëœ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ ì¬ì‘ì„±"""
    print(f"ğŸ›¡ï¸ AI í‘œì ˆ íšŒí”¼ ìš”ì²­: check_id={check_id}")
    
    try:
        # í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ
        service = PlagiarismService(db)
        check_result = service.get_check_result(check_id)
        
        if not check_result:
            raise HTTPException(status_code=404, detail="í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í‘œì ˆ ë§¤ì¹˜ ì •ë³´ ì§ì ‘ ì¡°íšŒ
        plagiarism_matches = db.query(PlagiarismMatch).filter(
            PlagiarismMatch.check_id == check_id
        ).all()
        
        # ë§¤ì¹˜ ì •ë³´ë¥¼ AI íšŒí”¼ ì‹œìŠ¤í…œì— ë§ê²Œ ë³€í™˜
        formatted_matches = []
        for match in plagiarism_matches:
            formatted_matches.append({
                "matched_text": match.matched_text,
                "start_index": match.start_index,
                "end_index": match.end_index,
                "similarity_score": match.similarity_score,
                "source_title": match.source_title
            })
        
        # AI í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì‹¤í–‰
        avoidance_system = AIPlagiarismAvoidance()
        avoidance_result = avoidance_system.avoid_plagiarism(
            check_result.original_text, 
            formatted_matches
        )
        
        return {
            "success": True,
            "check_id": check_id,
            "original_text": avoidance_result.original_text,
            "rewritten_text": avoidance_result.rewritten_text,
            "similarity_reduction": avoidance_result.similarity_reduction,
            "confidence_score": avoidance_result.confidence_score,
            "modifications": avoidance_result.modifications,
            "statistics": {
                "total_modifications": len(avoidance_result.modifications),
                "plagiarism_rewrites": len([m for m in avoidance_result.modifications if m["type"] == "plagiarism_rewrite"]),
                "general_variations": len([m for m in avoidance_result.modifications if m["type"] == "general_variation"]),
                "original_similarity": check_result.similarity_score,
                "estimated_new_similarity": max(0, check_result.similarity_score - avoidance_result.similarity_reduction)
            },
            "message": f"AIê°€ {len(avoidance_result.modifications)}ê°œ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ {avoidance_result.similarity_reduction:.1f}% ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"AI í‘œì ˆ íšŒí”¼ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI í‘œì ˆ íšŒí”¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.post("/avoid-plagiarism/text")
async def avoid_plagiarism_direct(
    request: Request,
    db: Session = Depends(get_db)
):
    """í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ìœ¼ë¡œ AI í‘œì ˆ íšŒí”¼"""
    payload = await request.json()
    text = payload.get("text", "")
    similarity_threshold = float(payload.get("similarity_threshold", 30.0))
    print(f"ğŸ›¡ï¸ ì§ì ‘ í…ìŠ¤íŠ¸ AI í‘œì ˆ íšŒí”¼: {len(text)}ì, ì„ê³„ê°’={similarity_threshold}%")
    
    try:
        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 10ì)")
        
        # ì‹¤ì œ í‘œì ˆ ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ í‘œì ˆ ë§¤ì¹˜ ì°¾ê¸°
        service = PlagiarismService(db)
        check_id = str(uuid.uuid4())
        
        # í‘œì ˆ ê²€ì‚¬ ìˆ˜í–‰
        check = service.create_check(check_id, text.strip())
        service.process_plagiarism_check(check_id, text.strip())
        
        # ë§¤ì¹˜ ì •ë³´ ì¡°íšŒ
        plagiarism_matches = db.query(PlagiarismMatch).filter(
            PlagiarismMatch.check_id == check_id
        ).all()
        
        # ë§¤ì¹˜ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        sample_matches = []
        for match in plagiarism_matches:
            if match.similarity_score >= similarity_threshold:
                sample_matches.append({
                    "matched_text": match.matched_text,
                    "start_index": match.start_index,
                    "end_index": match.end_index,
                    "similarity_score": match.similarity_score,
                    "source_title": match.source_title
                })
        
        # [*] ë§¤ì¹˜ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
        if not sample_matches:
            sentences = text.split('.')
            current_pos = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 10:  # ìµœì†Œ ê¸¸ì´
                    sample_matches.append({
                        "matched_text": sentence,
                        "start_index": current_pos,
                        "end_index": current_pos + len(sentence),
                        "similarity_score": 60.0,  # ì¤‘ê°„ ì •ë„ ìœ ì‚¬ë„
                        "source_title": "Knowledge Base"
                    })
                current_pos += len(sentence) + 1  # +1 for the period
        
        # AI í‘œì ˆ íšŒí”¼ ì‹¤í–‰
        avoidance_system = AIPlagiarismAvoidance()
        avoidance_result = avoidance_system.avoid_plagiarism(text.strip(), sample_matches)
        
        return {
            "success": True,
            "needs_rewriting": True,
            "original_text": avoidance_result.original_text,
            "rewritten_text": avoidance_result.rewritten_text,
            "similarity_reduction": avoidance_result.similarity_reduction,
            "confidence_score": avoidance_result.confidence_score,
            "modifications": avoidance_result.modifications,
            "plagiarism_check": {
                "original_similarity": 45.0,
                "total_matches": len(sample_matches),
                "high_risk_matches": 1,
                "estimated_new_similarity": max(0, 45.0 - avoidance_result.similarity_reduction)
            },
            "statistics": {
                "total_modifications": len(avoidance_result.modifications),
                "plagiarism_rewrites": len([m for m in avoidance_result.modifications if m["type"] == "plagiarism_rewrite"]),
                "general_variations": len([m for m in avoidance_result.modifications if m["type"] == "general_variation"])
            },
            "message": f"í‘œì ˆ ìœ„í—˜ í…ìŠ¤íŠ¸ë¥¼ AIê°€ ì¬ì‘ì„±í–ˆìŠµë‹ˆë‹¤. ì˜ˆìƒ ìœ ì‚¬ë„ ê°ì†Œ: {avoidance_result.similarity_reduction:.1f}%"
        }
        
    except Exception as e:
        print(f"ì§ì ‘ í…ìŠ¤íŠ¸ í‘œì ˆ íšŒí”¼ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ í‘œì ˆ íšŒí”¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/avoid-plagiarism/capabilities")
async def get_avoidance_capabilities():
    """AI í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ ê¸°ëŠ¥ ì†Œê°œ"""
    avoidance_system = AIPlagiarismAvoidance()
    stats = avoidance_system.get_avoidance_statistics()
    
    return {
        "success": True,
        "system_info": {
            "name": "AI í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ",
            "description": "í‘œì ˆ ìœ„í—˜ í…ìŠ¤íŠ¸ë¥¼ AIê°€ ìë™ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ë‚®ì¶¤",
            "version": "1.0",
            "author": "CopyKiller AI"
        },
        "capabilities": stats,
        "usage_guide": [
            "1. í‘œì ˆ ê²€ì‚¬ í›„ ê²°ê³¼ í˜ì´ì§€ì—ì„œ 'í‘œì ˆ íšŒí”¼' ë²„íŠ¼ í´ë¦­",
            "2. ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ ì¦‰ì‹œ íšŒí”¼ ì²˜ë¦¬",
            "3. AIê°€ ìë™ìœ¼ë¡œ í‘œì ˆ ë¶€ë¶„ì„ ê°ì§€í•˜ê³  ì¬ì‘ì„±",
            "4. ì›ë³¸ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ìœ ì‚¬ë„ë§Œ ë‚®ì¶¤"
        ],
        "features": [
            "í‘œì ˆ ë¶€ë¶„ ìë™ ê°ì§€",
            "ì˜ë¯¸ ë³´ì¡´ ì¬ì‘ì„±", 
            "ë‹¤ì–‘í•œ ë³€í™˜ ê¸°ë²•",
            "ìœ ì‚¬ë„ ê°ì†Œ ì˜ˆì¸¡",
            "ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ"
        ]
    }

# ==================== AI í‘œì ˆ íšŒí”¼ ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/ai-fix/plagiarism")
async def fix_plagiarism_automatically(
    request: Request,
    db: Session = Depends(get_db)
):
    """AI ê¸°ë°˜ ìë™ í‘œì ˆ íšŒí”¼ - ìœ ì‚¬ë„ê°€ ë†’ì€ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •"""
    payload = await request.json()
    text = payload.get("text", "")
    plagiarism_matches = payload.get("plagiarism_matches", [])
    print(f"ğŸ¤– AI í‘œì ˆ íšŒí”¼ ìš”ì²­: í…ìŠ¤íŠ¸ {len(text)}ì, ë§¤ì¹˜ {len(plagiarism_matches)}ê°œ")
    
    try:
        if not text or len(text.strip()) < 10:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
        
        if not plagiarism_matches:
            return {
                "success": True,
                "message": "í‘œì ˆëœ ë¶€ë¶„ì´ ì—†ì–´ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤",
                "original_text": text,
                "fixed_text": text,
                "fixes_applied": [],
                "similarity_improvement": "0%"
            }
        
        # AI í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        fixer = AIPlagiarismFixer()
        
        # í‘œì ˆ ë¶€ë¶„ ìë™ ìˆ˜ì •
        fixes = fixer.fix_plagiarized_text(text, plagiarism_matches)
        
        if not fixes:
            return {
                "success": True,
                "message": "ìˆ˜ì • ê°€ëŠ¥í•œ ê³ ìœ ì‚¬ë„ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤ (90% ì´ìƒ ìœ ì‚¬ë„ë§Œ ìˆ˜ì •)",
                "original_text": text,
                "fixed_text": text,
                "fixes_applied": [],
                "similarity_improvement": "0%"
            }
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì— ìˆ˜ì •ì‚¬í•­ ì ìš©
        fixed_text = fixer.apply_fixes_to_full_text(text, fixes)
        
        # ìˆ˜ì • ë³´ê³ ì„œ ìƒì„±
        fix_report = fixer.generate_fix_report(fixes)
        
        # ì „ì²´ ìœ ì‚¬ë„ ê°œì„  ê³„ì‚°
        total_improvement = sum(fix.similarity_before - fix.similarity_after for fix in fixes)
        
        return {
            "success": True,
            "message": f"AIê°€ {len(fixes)}ê°œ êµ¬ê°„ì„ ìë™ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤",
            "original_text": text,
            "fixed_text": fixed_text,
            "fixes_applied": fix_report["fixes"],
            "summary": {
                "total_fixes": fix_report["total_fixes"],
                "average_similarity_reduction": f"{fix_report['average_similarity_reduction']:.1%}",
                "total_similarity_improvement": f"{total_improvement:.1%}"
            },
            "ai_techniques_used": [
                "ë™ì˜ì–´ êµì²´",
                "ë¬¸ì¥ êµ¬ì¡° ë³€ê²½", 
                "í‘œí˜„ ë°©ì‹ ì „í™˜",
                "ë¬¸ì¥ ìˆœì„œ ì¡°ì •"
            ]
        }
        
    except Exception as e:
        print(f"AI í‘œì ˆ íšŒí”¼ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI í‘œì ˆ íšŒí”¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.post("/ai-fix/check/{check_id}")
async def fix_plagiarism_by_check_id(
    check_id: str,
    db: Session = Depends(get_db)
):
    """í‘œì ˆ ê²€ì‚¬ IDë¡œ AI ìë™ í‘œì ˆ íšŒí”¼"""
    print(f"ğŸ”§ ê²€ì‚¬ ID ê¸°ë°˜ AI í‘œì ˆ íšŒí”¼: {check_id}")
    
    try:
        service = PlagiarismService(db)
        
        # í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ
        check_result = service.get_check_result(check_id)
        if not check_result:
            raise HTTPException(status_code=404, detail="í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í‘œì ˆ ë§¤ì¹˜ ì¡°íšŒ
        plagiarism_matches = service.get_plagiarism_matches(check_id)
        
        # ê³ ìœ ì‚¬ë„ ë§¤ì¹˜ë§Œ í•„í„°ë§ (90% ì´ìƒ)
        high_similarity_matches = [
            {
                "start_index": match.start_index,
                "end_index": match.end_index, 
                "similarity_score": match.similarity_score,
                "matched_text": match.matched_text,
                "source_title": match.source_title
            }
            for match in plagiarism_matches 
            if match.similarity_score >= 0.90
        ]
        
        if not high_similarity_matches:
            return {
                "success": True,
                "message": "90% ì´ìƒ ê³ ìœ ì‚¬ë„ êµ¬ê°„ì´ ì—†ì–´ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤",
                "original_text": check_result.original_text,
                "fixed_text": check_result.original_text,
                "high_similarity_matches": 0,
                "fixes_applied": []
            }
        
        # AI í‘œì ˆ íšŒí”¼ ì ìš©
        fixer = AIPlagiarismFixer()
        fixes = fixer.fix_plagiarized_text(check_result.original_text, high_similarity_matches)
        
        if fixes:
            fixed_text = fixer.apply_fixes_to_full_text(check_result.original_text, fixes)
            fix_report = fixer.generate_fix_report(fixes)
            
            return {
                "success": True,
                "message": f"AIê°€ {len(fixes)}ê°œ ê³ ìœ ì‚¬ë„ êµ¬ê°„ì„ ìë™ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤",
                "original_text": check_result.original_text,
                "fixed_text": fixed_text,
                "check_info": {
                    "check_id": check_id,
                    "original_similarity": f"{check_result.similarity_score:.1%}",
                    "total_matches": len(plagiarism_matches),
                    "high_similarity_matches": len(high_similarity_matches)
                },
                "fixes_applied": fix_report["fixes"],
                "improvement_summary": {
                    "total_fixes": fix_report["total_fixes"],
                    "average_reduction": f"{fix_report['average_similarity_reduction']:.1%}",
                    "total_improvement": f"{fix_report['total_similarity_reduction']:.1%}"
                },
                "ai_recommendations": [
                    "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í‘œì ˆ ê²€ì‚¬í•´ë³´ì„¸ìš”",
                    "ì¶”ê°€ì ì¸ ìˆ˜ë™ í¸ì§‘ìœ¼ë¡œ ë” ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                    "ë¬¸ë§¥ê³¼ ì˜ë¯¸ê°€ ìœ ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
                ]
            }
        else:
            return {
                "success": True,
                "message": "ìë™ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                "original_text": check_result.original_text,
                "fixed_text": check_result.original_text,
                "fixes_applied": [],
                "recommendation": "ìˆ˜ë™ í¸ì§‘ì„ í†µí•œ ê°œì„ ì„ ê¶Œì¥í•©ë‹ˆë‹¤"
            }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"ê²€ì‚¬ ID ê¸°ë°˜ í‘œì ˆ íšŒí”¼ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI í‘œì ˆ íšŒí”¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/ai-fix/capabilities")
async def get_ai_fix_capabilities():
    """AI í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ ê¸°ëŠ¥ ì†Œê°œ"""
    return {
        "success": True,
        "ai_fix_system": {
            "name": "AI ìë™ í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ",
            "description": "ìœ ì‚¬ë„ê°€ ë†’ì€ êµ¬ê°„ì„ AIê°€ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ í‘œì ˆì„ íšŒí”¼í•˜ë„ë¡ ìˆ˜ì •",
            "target_similarity": "90% ì´ìƒ ê³ ìœ ì‚¬ë„ êµ¬ê°„",
            "techniques": [
                {
                    "name": "ì§€ëŠ¥í˜• ë™ì˜ì–´ êµì²´",
                    "description": "ë¬¸ë§¥ì— ë§ëŠ” ìµœì ì˜ ë™ì˜ì–´ë¡œ ìë™ êµì²´"
                },
                {
                    "name": "ë¬¸ì¥ êµ¬ì¡° ë³€ê²½",
                    "description": "ìˆ˜ë™íƒœâ†”ëŠ¥ë™íƒœ, ë¬¸ì¥ ìˆœì„œ ì¡°ì • ë“±"
                },
                {
                    "name": "í‘œí˜„ ë°©ì‹ ì „í™˜",
                    "description": "í•™ìˆ ì  í‘œí˜„ ê°•í™”, ì–´ì¡° ë³€ê²½ ë“±"
                },
                {
                    "name": "ë¬¸ì¥ ìˆœì„œ ì¡°ì •",
                    "description": "ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¬¸ì¥ ë°°ì¹˜ ë³€ê²½"
                }
            ],
            "features": [
                "ì‹¤ì‹œê°„ ìë™ ìˆ˜ì •",
                "ìœ ì‚¬ë„ ì˜ˆì¸¡",
                "ìˆ˜ì • ë³´ê³ ì„œ ìƒì„±",
                "ë‹¤ì¤‘ ê¸°ë²• ì ìš©",
                "ì›ë³¸ ì˜ë¯¸ ë³´ì¡´"
            ],
            "supported_content": [
                "í•™ìˆ  ë…¼ë¬¸",
                "ë³´ê³ ì„œ",
                "ì—ì„¸ì´",
                "ì—°êµ¬ ìë£Œ",
                "ì¼ë°˜ í…ìŠ¤íŠ¸"
            ]
        },
        "usage_workflow": [
            "1. í‘œì ˆ ê²€ì‚¬ ì‹¤í–‰",
            "2. 90% ì´ìƒ ìœ ì‚¬ë„ êµ¬ê°„ ê°ì§€", 
            "3. AI ìë™ ìˆ˜ì • ì ìš©",
            "4. ìˆ˜ì • ê²°ê³¼ ë° ë³´ê³ ì„œ í™•ì¸",
            "5. í•„ìš”ì‹œ ì¶”ê°€ ìˆ˜ë™ í¸ì§‘"
        ]
    }

# ==================== ë¬¸ì¥ ê°œì„  ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/improve/check/{check_id}")
async def get_sentence_improvements(
    check_id: str,
    db: Session = Depends(get_db)
):
    """í‘œì ˆ ê²€ì‚¬ ê²°ê³¼ì— ëŒ€í•œ ë¬¸ì¥ ê°œì„  ì œì•ˆ"""
    try:
        # ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ
        check = db.query(PlagiarismCheck).filter(
            PlagiarismCheck.check_id == check_id
        ).first()
        
        if not check:
            raise HTTPException(status_code=404, detail="ê²€ì‚¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë§¤ì¹˜ ì¡°íšŒ
        matches = db.query(PlagiarismMatch).filter(
            PlagiarismMatch.check_id == check_id
        ).all()
        
        if not matches:
            return {
                "success": True,
                "message": "í‘œì ˆ ë¶€ë¶„ì´ ì—†ì–´ ê°œì„ ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                "improvement_data": {
                    "suggestions": [],
                    "summary": "ì™„ë²½í•œ ì›ë¬¸ì…ë‹ˆë‹¤"
                }
            }
        
        # SentenceImprovementServiceë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì„  ì œì•ˆ ìƒì„±
        improvement_service = SentenceImprovementService()
        suggestions = []
        
        for match in matches[:5]:  # ìƒìœ„ 5ê°œ ë§¤ì¹˜ë§Œ ì²˜ë¦¬
            if match.similarity_score >= 50:
                improved = improvement_service.improve_sentence(match.matched_text)
                if improved:
                    suggestions.append({
                        "original": match.matched_text,
                        "improved": improved["improved_text"],
                        "type": improved.get("improvement_type", "íŒ¨ëŸ¬í”„ë ˆì´ì§•"),
                        "explanation": improved.get("explanation", ""),
                        "similarity_reduction": f"{improved.get('estimated_similarity_reduction', 15)}%"
                    })
        
        return {
            "success": True,
            "message": f"{len(suggestions)}ê°œì˜ ê°œì„  ì œì•ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤",
            "improvement_data": {
                "suggestions": suggestions,
                "summary": f"ì´ {len(suggestions)}ê°œ êµ¬ê°„ ê°œì„  ê°€ëŠ¥",
                "total_matches": len(matches),
                "high_similarity_matches": sum(1 for m in matches if m.similarity_score >= 80)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"ë¬¸ì¥ ê°œì„  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë¬¸ì¥ ê°œì„  ì¤‘ ì˜¤ë¥˜: {str(e)}")