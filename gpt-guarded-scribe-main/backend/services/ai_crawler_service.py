#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""AI ê¸°ë°˜ ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""

import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import re
import time
import json
from urllib.parse import urljoin, urlparse, quote
from typing import List, Dict, Optional
import random
from dataclasses import dataclass

@dataclass
class CrawlTarget:
    """í¬ë¡¤ë§ ëŒ€ìƒ ì •ë³´"""
    domain: str
    name: str
    search_url_pattern: str
    content_selectors: List[str]
    title_selectors: List[str]
    requires_js: bool = False

class AICrawlerService:
    """AI ê¸°ë°˜ ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db_path="plagiarism.db"):
        self.db_path = db_path
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ë‹¤ì–‘í•œ í•œêµ­ì–´ ì»¨í…ì¸  ì†ŒìŠ¤ ì •ì˜
        self.crawl_targets = {
            'wikipedia': CrawlTarget(
                domain='ko.wikipedia.org',
                name='ìœ„í‚¤ë°±ê³¼',
                search_url_pattern='https://ko.wikipedia.org/w/api.php?action=opensearch&search={}&limit=10&format=json',
                content_selectors=['.mw-parser-output', '#mw-content-text', '.mw-content-ltr'],
                title_selectors=['h1.firstHeading', '.mw-page-title-main', 'h1']
            ),
            'namuwiki': CrawlTarget(
                domain='namu.wiki',
                name='ë‚˜ë¬´ìœ„í‚¤',
                search_url_pattern='https://namu.wiki/Search?q={}',
                content_selectors=['.wiki-content', '.wiki-article', '#app'],
                title_selectors=['.wiki-title', 'h1', '.title']
            ),
            'naver_encyclopedia': CrawlTarget(
                domain='terms.naver.com',
                name='ë„¤ì´ë²„ ì§€ì‹ë°±ê³¼',
                search_url_pattern='https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=site:terms.naver.com {}',
                content_selectors=['.se_component', '.api_cs_wrap', '.content_area'],
                title_selectors=['.headword', 'h2', '.title']
            ),
            'doopedia': CrawlTarget(
                domain='www.doopedia.co.kr',
                name='ë‘ì‚°ë°±ê³¼',
                search_url_pattern='https://www.doopedia.co.kr/search/encyber/result.do?query={}',
                content_selectors=['.viewcon', '.cont', '.content'],
                title_selectors=['.tit', 'h1', '.title']
            ),
            'kpedia': CrawlTarget(
                domain='kpedia.jp',
                name='í•œêµ­ì–´ ìœ„í‚¤',
                search_url_pattern='https://kpedia.jp/s/{}/1',
                content_selectors=['.content', '.article', '.main'],
                title_selectors=['.title', 'h1', 'h2']
            ),
            'korean_history': CrawlTarget(
                domain='contents.history.go.kr',
                name='í•œêµ­ì‚¬ ì½˜í…ì¸ ',
                search_url_pattern='https://contents.history.go.kr/mobile/ka/search.do?keywords={}',
                content_selectors=['.cont_area', '.content', '.article'],
                title_selectors=['.tit', 'h1', '.title']
            ),
            'korean_culture': CrawlTarget(
                domain='encykorea.aks.ac.kr',
                name='í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼',
                search_url_pattern='https://encykorea.aks.ac.kr/search/SearchList.do?keyword={}',
                content_selectors=['.content', '.view_content', '.article_content'],
                title_selectors=['.view_title', 'h1', '.title']
            ),
            'science_all': CrawlTarget(
                domain='www.scienceall.com',
                name='ì‚¬ì´ì–¸ìŠ¤ì˜¬',
                search_url_pattern='https://www.scienceall.com/search/?q={}',
                content_selectors=['.content', '.article-content', '.entry-content'],
                title_selectors=['.entry-title', 'h1', '.title']
            )
        }
    
    def intelligent_search(self, query: str, num_results: int = 10) -> List[Dict]:
        """AI ê¸°ë°˜ ì§€ëŠ¥í˜• ê²€ìƒ‰ ë° í¬ë¡¤ë§"""
        print(f"ğŸ¤– AI ê¸°ë°˜ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        all_articles = []
        
        # 1. í‚¤ì›Œë“œ í™•ì¥ ë° ë‹¤ì–‘í™”
        expanded_queries = self._expand_search_queries(query)
        print(f"ğŸ“ í™•ì¥ëœ ê²€ìƒ‰ì–´: {expanded_queries}")
        
        # 2. ê° ì†ŒìŠ¤ë³„ë¡œ í¬ë¡¤ë§
        for target_name, target in self.crawl_targets.items():
            print(f"\nğŸ¯ {target.name} í¬ë¡¤ë§ ì¤‘...")
            
            for search_query in expanded_queries[:3]:  # ìƒìœ„ 3ê°œ ê²€ìƒ‰ì–´ë§Œ ì‚¬ìš©
                try:
                    articles = self._crawl_from_source(target, search_query, max_articles=3)
                    all_articles.extend(articles)
                    print(f"âœ… {target.name}: {len(articles)}ê°œ ìˆ˜ì§‘")
                    
                    # ìš”ì²­ ê°„ ì§€ì—° (ì°¨ë‹¨ ë°©ì§€)
                    time.sleep(random.uniform(1, 3))
                    
                except Exception as e:
                    print(f"âŒ {target.name} ì˜¤ë¥˜: {e}")
                    continue
        
        # 3. ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        filtered_articles = self._filter_and_deduplicate(all_articles)
        
        # 4. ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        return filtered_articles[:num_results]
    
    def _expand_search_queries(self, original_query: str) -> List[str]:
        """ê²€ìƒ‰ì–´ í™•ì¥ ë° ë‹¤ì–‘í™”"""
        queries = [original_query]
        
        # ë™ì˜ì–´ ë° ê´€ë ¨ì–´ ì‚¬ì „
        expansion_dict = {
            'ì¸ê³µì§€ëŠ¥': ['AI', 'ë¨¸ì‹ ëŸ¬ë‹', 'ê¸°ê³„í•™ìŠµ', 'ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§', 'ì•Œê³ ë¦¬ì¦˜'],
            'ê¸°í›„ë³€í™”': ['ì§€êµ¬ì˜¨ë‚œí™”', 'íƒ„ì†Œì¤‘ë¦½', 'ì˜¨ì‹¤ê°€ìŠ¤', 'í™˜ê²½', 'ê¸°í›„ìœ„ê¸°', 'ì¹œí™˜ê²½'],
            'êµìœ¡': ['í•™ìŠµ', 'í•™êµ', 'êµìœ¡ê³¼ì •', 'êµìœ¡ì œë„', 'êµìœ¡ì •ì±…', 'í•™êµêµìœ¡'],
            'ê²½ì œ': ['ê²½ì œí•™', 'ì‹œì¥', 'ê¸ˆìœµ', 'íˆ¬ì', 'ê²½ì œì •ì±…', 'ê²½ì œì„±ì¥'],
            'ê±´ê°•': ['ì˜í•™', 'ì§ˆë³‘', 'ì¹˜ë£Œ', 'ì˜ˆë°©', 'ì˜ë£Œ', 'ë³´ê±´'],
            'ê¸°ìˆ ': ['í…Œí¬ë†€ë¡œì§€', 'í˜ì‹ ', 'ê³¼í•™ê¸°ìˆ ', 'IT', 'ë””ì§€í„¸', 'ì²¨ë‹¨ê¸°ìˆ '],
            'ì‚¬íšŒ': ['ì‚¬íšŒí•™', 'ê³µë™ì²´', 'ì‚¬íšŒë¬¸ì œ', 'ì‚¬íšŒì œë„', 'ì‚¬íšŒë³€í™”'],
            'ì •ì¹˜': ['ì •ë¶€', 'ì •ì±…', 'ë¯¼ì£¼ì£¼ì˜', 'ì •ì¹˜ì œë„', 'í–‰ì •', 'êµ­ì •'],
            'ë¬¸í™”': ['ì˜ˆìˆ ', 'ì „í†µ', 'ë¬¸í™”ì˜ˆìˆ ', 'í•œêµ­ë¬¸í™”', 'ë¬¸í™”ìœ ì‚°', 'ëŒ€ì¤‘ë¬¸í™”'],
            'ì—­ì‚¬': ['í•œêµ­ì‚¬', 'ì„¸ê³„ì‚¬', 'ì—­ì‚¬í•™', 'ì „í†µ', 'ë¬¸í™”ì¬', 'ìœ ì ']
        }
        
        # ê´€ë ¨ì–´ ì¶”ê°€
        for keyword, related_terms in expansion_dict.items():
            if keyword in original_query:
                queries.extend(related_terms[:3])
        
        # ë³µí•© ê²€ìƒ‰ì–´ ìƒì„±
        if len(original_query.split()) == 1:
            compound_queries = [
                f"{original_query} ê°œë…",
                f"{original_query} ì •ì˜",
                f"{original_query} íŠ¹ì§•",
                f"{original_query} í˜„í™©",
                f"{original_query} ë™í–¥"
            ]
            queries.extend(compound_queries)
        
        return list(set(queries))[:10]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 10ê°œ
    
    def _crawl_from_source(self, target: CrawlTarget, query: str, max_articles: int = 5) -> List[Dict]:
        """íŠ¹ì • ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§"""
        articles = []
        
        try:
            # ê²€ìƒ‰ URL ìƒì„±
            search_url = target.search_url_pattern.format(quote(query))
            
            # ìœ„í‚¤ë°±ê³¼ API íŠ¹ë³„ ì²˜ë¦¬
            if target.domain == 'ko.wikipedia.org':
                return self._crawl_wikipedia_api(query, max_articles)
            
            # ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
            response = requests.get(search_url, headers=self.headers, timeout=15)
            if response.status_code != 200:
                return articles
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ ì¶”ì¶œ
            links = self._extract_search_result_links(soup, target)
            
            # ê° ë§í¬ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
            for link in links[:max_articles]:
                try:
                    article = self._extract_article_content(link, target)
                    if article:
                        articles.append(article)
                    time.sleep(random.uniform(0.5, 1.5))
                except Exception as e:
                    print(f"âš ï¸  ë§í¬ ì²˜ë¦¬ ì‹¤íŒ¨ {link}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âŒ {target.name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return articles
    
    def _crawl_wikipedia_api(self, query: str, max_articles: int = 5) -> List[Dict]:
        """ìœ„í‚¤ë°±ê³¼ APIë¥¼ í†µí•œ í¬ë¡¤ë§"""
        articles = []
        
        try:
            # OpenSearch APIë¡œ ê²€ìƒ‰
            search_url = f"https://ko.wikipedia.org/w/api.php?action=opensearch&search={quote(query)}&limit={max_articles}&format=json"
            response = requests.get(search_url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                titles = data[1] if len(data) > 1 else []
                urls = data[3] if len(data) > 3 else []
                
                for title, url in zip(titles, urls):
                    try:
                        # ê° í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        content_response = requests.get(url, headers=self.headers, timeout=10)
                        if content_response.status_code == 200:
                            soup = BeautifulSoup(content_response.text, 'html.parser')
                            
                            # ë³¸ë¬¸ ì¶”ì¶œ
                            content_element = soup.select_one('.mw-parser-output')
                            if content_element:
                                content = self._clean_text(content_element.get_text())
                                if len(content) > 200:
                                    articles.append({
                                        'title': title,
                                        'content': content,
                                        'url': url,
                                        'source_type': 'wikipedia',
                                        'source_name': 'ìœ„í‚¤ë°±ê³¼'
                                    })
                        
                        time.sleep(0.5)
                        
                    except Exception as e:
                        print(f"âš ï¸  ìœ„í‚¤ë°±ê³¼ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {title}: {e}")
                        continue
                        
        except Exception as e:
            print(f"âŒ ìœ„í‚¤ë°±ê³¼ API ì˜¤ë¥˜: {e}")
            
        return articles
    
    def _extract_search_result_links(self, soup: BeautifulSoup, target: CrawlTarget) -> List[str]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        # ì¼ë°˜ì ì¸ ë§í¬ ì„ íƒìë“¤
        link_selectors = ['a[href]', '.result a', '.search-result a', '.title a']
        
        for selector in link_selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href', '')
                if href:
                    # ìƒëŒ€ ë§í¬ë¥¼ ì ˆëŒ€ ë§í¬ë¡œ ë³€í™˜
                    if href.startswith('/'):
                        href = f"https://{target.domain}{href}"
                    elif href.startswith('http'):
                        # ë™ì¼ ë„ë©”ì¸ì¸ì§€ í™•ì¸
                        if target.domain in href:
                            links.append(href)
                    
            if len(links) >= 10:
                break
                
        return list(set(links))[:10]
    
    def _extract_article_content(self, url: str, target: CrawlTarget) -> Optional[Dict]:
        """ê°œë³„ ê¸°ì‚¬ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = None
            for selector in target.title_selectors:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text().strip()
                    break
            
            if not title:
                title = soup.title.get_text().strip() if soup.title else "ì œëª© ì—†ìŒ"
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = None
            for selector in target.content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = self._clean_text(element.get_text())
                    if len(content) > 200:
                        break
            
            if not content or len(content) < 200:
                return None
            
            return {
                'title': title[:200],
                'content': content,
                'url': url,
                'source_type': 'crawled',
                'source_name': target.name
            }
            
        except Exception as e:
            print(f"âŒ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def _filter_and_deduplicate(self, articles: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§"""
        # URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('url', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        # ì½˜í…ì¸  ê¸¸ì´ ê¸°ì¤€ í•„í„°ë§
        quality_articles = [
            article for article in unique_articles 
            if len(article.get('content', '')) >= 500
        ]
        
        # ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•œ ë°©ì‹)
        final_articles = []
        seen_titles = set()
        
        for article in quality_articles:
            title = article.get('title', '').lower()
            title_key = ''.join(title.split()[:3])  # ì²« 3ë‹¨ì–´ë¡œ ìœ ì‚¬ë„ íŒë‹¨
            
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                final_articles.append(article)
        
        return final_articles
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•œêµ­ì–´ ë³´ì¡´)
        text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?():;-]', '', text)
        # ê¸¸ì´ ì œí•œ
        return text.strip()[:8000]
    
    def save_to_database(self, articles: List[Dict]) -> int:
        """í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not articles:
            return 0
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
            saved_count = 0
            
            for article in articles:
                # ì¤‘ë³µ ì²´í¬ (URLê³¼ ì œëª© ëª¨ë‘)
                cursor.execute("""
                    SELECT id FROM document_sources 
                    WHERE (url = ? OR title = ?) AND is_active = 1
                """, (article['url'], article['title']))
                
                if cursor.fetchone():
                    print(f"âš ï¸  ì´ë¯¸ ì¡´ì¬: {article['title'][:50]}...")
                    continue
                
                # ì €ì¥
                cursor.execute("""
                    INSERT INTO document_sources 
                    (title, content, url, source_type, created_at, updated_at, is_active)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['title'],
                    article['content'],
                    article['url'],
                    f"{article['source_type']}_{article.get('source_name', 'unknown')}",
                    current_time,
                    current_time,
                    1
                ))
                saved_count += 1
                print(f"ğŸ’¾ ì €ì¥ë¨: [{article.get('source_name', 'Unknown')}] {article['title'][:50]}...")
            
            conn.commit()
            conn.close()
            
            return saved_count
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return 0
    
    def ai_enhanced_crawl(self, query: str, num_results: int = 15) -> Dict:
        """AI ê°•í™” í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸš€ AI ê°•í™” ì›¹ í¬ë¡¤ë§ ì‹œì‘: '{query}'")
        print(f"ğŸ¯ ëŒ€ìƒ ì†ŒìŠ¤: {len(self.crawl_targets)}ê°œ ì‚¬ì´íŠ¸")
        
        # ì§€ëŠ¥í˜• ê²€ìƒ‰ ë° í¬ë¡¤ë§
        articles = self.intelligent_search(query, num_results)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_count = self.save_to_database(articles)
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'query': query,
            'total_crawled': len(articles),
            'saved_count': saved_count,
            'sources_used': list(set([article.get('source_name', 'Unknown') for article in articles])),
            'articles': [
                {
                    'title': article['title'][:100],
                    'content_length': len(article['content']),
                    'url': article['url'],
                    'source': article.get('source_name', 'Unknown')
                }
                for article in articles
            ]
        }
        
        print(f"âœ… AI í¬ë¡¤ë§ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì´ ìˆ˜ì§‘: {len(articles)}ê°œ")
        print(f"   ğŸ’¾ ì €ì¥: {saved_count}ê°œ")
        print(f"   ğŸŒ ì‚¬ìš© ì†ŒìŠ¤: {', '.join(result['sources_used'])}")
        
        return result

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    crawler = AICrawlerService()
    
    # ë‹¤ì–‘í•œ ì£¼ì œë¡œ AI í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
    test_queries = ["ì¸ê³µì§€ëŠ¥", "ê¸°í›„ë³€í™”", "í•œêµ­ì‚¬", "ê²½ì œí•™"]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        result = crawler.ai_enhanced_crawl(query, 10)
        print(f"\nğŸ“ˆ ê²°ê³¼ ìš”ì•½:")
        print(f"   ê²€ìƒ‰ì–´: {result['query']}")
        print(f"   ìˆ˜ì§‘: {result['total_crawled']}ê°œ")
        print(f"   ì €ì¥: {result['saved_count']}ê°œ")
        print(f"   ì†ŒìŠ¤: {', '.join(result['sources_used'])}")
        print("="*60)