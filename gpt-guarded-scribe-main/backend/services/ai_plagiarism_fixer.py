#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""AI ê¸°ë°˜ í‘œì ˆ íšŒí”¼ ì„œë¹„ìŠ¤ - ìœ ì‚¬ë„ê°€ ë†’ì€ ë¶€ë¶„ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •"""

from typing import Dict, List, Optional, Tuple
import re
from dataclasses import dataclass
import random

@dataclass
class PlagiarismFix:
    """í‘œì ˆ ìˆ˜ì • ê²°ê³¼"""
    original_segment: str
    fixed_segment: str
    similarity_before: float
    similarity_after: float  # ì˜ˆìƒ ìœ ì‚¬ë„
    fix_type: str
    confidence: float
    start_index: int
    end_index: int

class AIPlagiarismFixer:
    """AI ê¸°ë°˜ í‘œì ˆ íšŒí”¼ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ê³ ê¸‰ ë™ì˜ì–´ ì‚¬ì „ (í‘œì ˆ íšŒí”¼ìš©)
        self.plagiarism_synonyms = {
            # í•™ìˆ  ìš©ì–´
            "ì—°êµ¬": ["ì¡°ì‚¬", "íƒêµ¬", "ë¶„ì„", "ê²€í† ", "ê³ ì°°", "ì—°êµ¬ì¡°ì‚¬", "í•™ìˆ ì—°êµ¬"],
            "ë¶„ì„": ["ê²€í† ", "ê³ ì°°", "íƒêµ¬", "í•´ì„", "í‰ê°€", "ì¡°ì‚¬ë¶„ì„", "ì‹¬ì¸µë¶„ì„"],
            "ê²°ê³¼": ["ì„±ê³¼", "ì‚°ì¶œ", "ë„ì¶œ", "ê·€ê²°", "ê²°ë¡ ", "ì—°êµ¬ê²°ê³¼", "ë¶„ì„ê²°ê³¼"],
            "ë°©ë²•": ["ìˆ˜ë‹¨", "ë°©ì‹", "ê¸°ë²•", "ì ‘ê·¼ë²•", "ì ˆì°¨", "ì—°êµ¬ë°©ë²•", "ë¶„ì„ë°©ë²•"],
            "ì´ë¡ ": ["í•™ì„¤", "ê°€ì„¤", "ê°œë…", "ì›ë¦¬", "ì´ë…", "ì´ë¡ ì²´ê³„", "í•™ìˆ ì´ë¡ "],
            "ëª¨ë¸": ["ëª¨í˜•", "í‹€", "ì²´ê³„", "êµ¬ì¡°", "íŒ¨í„´", "ë¶„ì„ëª¨ë¸", "ì´ë¡ ëª¨ë¸"],
            
            # ì¼ë°˜ ë™ì‚¬
            "ì œì‹œí•˜ë‹¤": ["ì œì•ˆí•˜ë‹¤", "ë‚´ì„¸ìš°ë‹¤", "ì£¼ì¥í•˜ë‹¤", "í‘œëª…í•˜ë‹¤", "ê±´ì˜í•˜ë‹¤", "ì–¸ê¸‰í•˜ë‹¤"],
            "ë‚˜íƒ€ë‚˜ë‹¤": ["ë“œëŸ¬ë‚˜ë‹¤", "ë³´ì´ë‹¤", "ë°œí˜„ë˜ë‹¤", "í‘œì¶œë˜ë‹¤", "ë‚˜ì˜¤ë‹¤", "ë³´ì—¬ì§€ë‹¤"],
            "ë³´ì—¬ì£¼ë‹¤": ["ë“œëŸ¬ë‚´ë‹¤", "ì œì‹œí•˜ë‹¤", "ë‚˜íƒ€ë‚´ë‹¤", "ì…ì¦í•˜ë‹¤", "ì‹œì‚¬í•˜ë‹¤", "ë³´ì—¬ì£¼ë‹¤"],
            "í™•ì¸í•˜ë‹¤": ["ê²€ì¦í•˜ë‹¤", "ì…ì¦í•˜ë‹¤", "ì¦ëª…í•˜ë‹¤", "íŒŒì•…í•˜ë‹¤", "ì•Œì•„ë³´ë‹¤", "ì ê²€í•˜ë‹¤"],
            "ë°œê²¬í•˜ë‹¤": ["ì°¾ì•„ë‚´ë‹¤", "ì•Œì•„ë‚´ë‹¤", "íŒŒì•…í•˜ë‹¤", "ê·œëª…í•˜ë‹¤", "ë°í˜€ë‚´ë‹¤", "ë„ì¶œí•˜ë‹¤"],
            "ì¦ê°€í•˜ë‹¤": ["ëŠ˜ì–´ë‚˜ë‹¤", "í™•ëŒ€ë˜ë‹¤", "ìƒìŠ¹í•˜ë‹¤", "í–¥ìƒë˜ë‹¤", "ì‹ ì¥ë˜ë‹¤", "íŒ½ì°½í•˜ë‹¤"],
            "ê°ì†Œí•˜ë‹¤": ["ì¤„ì–´ë“¤ë‹¤", "ì¶•ì†Œë˜ë‹¤", "í•˜ë½í•˜ë‹¤", "ì €í•˜ë˜ë‹¤", "ìœ„ì¶•ë˜ë‹¤", "ê°ì¶•ë˜ë‹¤"],
            
            # í˜•ìš©ì‚¬
            "ì¤‘ìš”í•œ": ["í•µì‹¬ì ì¸", "í•„ìˆ˜ì ì¸", "ê²°ì •ì ì¸", "ì£¼ìš”í•œ", "ì¤‘ëŒ€í•œ", "ì˜ë¯¸ìˆëŠ”", "ì¤‘ì°¨ëŒ€í•œ"],
            "íš¨ê³¼ì ì¸": ["ìœ íš¨í•œ", "íš¨ìœ¨ì ì¸", "ì„±ê³µì ì¸", "ìœ ìš©í•œ", "ì‹¤íš¨ì„±ìˆëŠ”", "íš¨ê³¼ìˆëŠ”"],
            "ìƒˆë¡œìš´": ["í˜ì‹ ì ì¸", "ì°¸ì‹ í•œ", "ìµœì‹ ì˜", "ì‹ ê·œ", "ì²¨ë‹¨", "ìƒˆë¡­ë‹¤", "ì‹ ì„ í•œ"],
            "ë‹¤ì–‘í•œ": ["ì—¬ëŸ¬", "ê°ì¢…", "ë‹¤ì¢…", "ê°–ê°€ì§€", "ì˜¨ê°–", "ì—¬ëŸ¬ê°€ì§€", "ë‹¤ì±„ë¡œìš´"],
            "ë³µì¡í•œ": ["ë³µí•©ì ì¸", "ë‹¤ë©´ì ì¸", "ë³µìˆ˜ì ì¸", "ë‹¤ì¸µì ì¸", "ì–´ë ¤ìš´", "ë‚œí•´í•œ"],
            
            # ì ‘ì†ì–´/ë¶€ì‚¬
            "ê·¸ëŸ¬ë‚˜": ["í•˜ì§€ë§Œ", "ë‹¤ë§Œ", "ë°˜ë©´ì—", "ê·¸ëŸ¼ì—ë„", "ê·¸ë ‡ì§€ë§Œ", "ê·¸ëŸ°ë°ë„"],
            "ë”°ë¼ì„œ": ["ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ëŸ° ì´ìœ ë¡œ", "ì´ì— ë”°ë¼", "ê²°ê³¼ì ìœ¼ë¡œ", "ê·¸ë¦¬í•˜ì—¬", "ë•Œë¬¸ì—"],
            "ë˜í•œ": ["ë”ë¶ˆì–´", "ì•„ìš¸ëŸ¬", "ë™ì‹œì—", "ë¿ë§Œ ì•„ë‹ˆë¼", "ê·¸ë¦¬ê³ ", "ê²Œë‹¤ê°€"],
            "íŠ¹íˆ": ["íŠ¹ë³„íˆ", "ë¬´ì—‡ë³´ë‹¤", "ì£¼ë¡œ", "íŠ¹íˆë‚˜", "ë”ìš±ì´", "ê·¸ì¤‘ì—ì„œë„"],
            "ì¦‰": ["ë‹¤ì‹œ ë§í•´", "ë°”ê¾¸ì–´ ë§í•˜ë©´", "ìš”ì•½í•˜ë©´", "êµ¬ì²´ì ìœ¼ë¡œ", "ë§í•˜ìë©´", "ì •ë¦¬í•˜ë©´"],
            
            # ëª…ì‚¬
            "ë¬¸ì œ": ["ê³¼ì œ", "ì´ìŠˆ", "ì‚¬ì•ˆ", "ìŸì ", "í˜„ì•ˆ", "ë¬¸ì œì ", "í•´ê²°ê³¼ì œ"],
            "ì‚¬íšŒ": ["ê³µë™ì²´", "ì§‘ë‹¨", "ì»¤ë®¤ë‹ˆí‹°", "ì‚¬íšŒêµ¬ì¡°", "ì‚¬íšŒì²´ê³„", "ì‚¬íšŒì§‘ë‹¨"],
            "ê¸°ìˆ ": ["í…Œí¬ë†€ë¡œì§€", "ê³µí•™", "ê¸°ë²•", "ë°©ë²•ë¡ ", "ë…¸í•˜ìš°", "ì²¨ë‹¨ê¸°ìˆ "],
            "ê²½ì œ": ["ê²½ì œí•™", "ê²½ì œì²´ê³„", "ê²½ì œêµ¬ì¡°", "ì‹œì¥", "ê²½ì œí™œë™", "ê²½ì œìƒí™©"],
            "êµìœ¡": ["í•™ìŠµ", "êµìœ¡ê³¼ì •", "êµìœ¡ì œë„", "êµìœ¡ì‹œìŠ¤í…œ", "í•™êµêµìœ¡", "êµìœ¡í™œë™"],
            "ì •ì¹˜": ["ì •ì¹˜í•™", "ì •ì¹˜ì œë„", "ì •ì¹˜ì²´ê³„", "ì •ë¶€", "í–‰ì •", "êµ­ì •ìš´ì˜"],
        }
        
        # ë¬¸ì¥ êµ¬ì¡° ë³€í™˜ íŒ¨í„´
        self.structure_patterns = {
            # ìˆ˜ë™íƒœ â†’ ëŠ¥ë™íƒœ
            "passive_to_active": [
                (r'(\w+)ì´ (\w+)ë˜ì—ˆë‹¤', r'\2ê°€ \1ì„ ì´ë£¨ì—ˆë‹¤'),
                (r'(\w+)ê°€ (\w+)ë˜ë‹¤', r'\1ì´ \2ë¥¼ ë§Œë“¤ë‹¤'),
                (r'(\w+)ì— ì˜í•´ (\w+)ë˜ë‹¤', r'\1ì´ \2ë¥¼ í•˜ë‹¤'),
                (r'(\w+)ìœ¼ë¡œ (\w+)ëœë‹¤', r'\1ì„ í†µí•´ \2í•œë‹¤'),
            ],
            
            # ëŠ¥ë™íƒœ â†’ ìˆ˜ë™íƒœ
            "active_to_passive": [
                (r'(\w+)ê°€ (\w+)ì„ (\w+)í•œë‹¤', r'\2ëŠ” \1ì— ì˜í•´ \3ëœë‹¤'),
                (r'(\w+)ì´ (\w+)ë¥¼ (\w+)í–ˆë‹¤', r'\2ê°€ \1ì— ì˜í•´ \3ë˜ì—ˆë‹¤'),
            ],
            
            # ë¬¸ì¥ ìˆœì„œ ë³€ê²½
            "order_change": [
                (r'(\w+)í•˜ê¸° ìœ„í•´ (\w+)í•œë‹¤', r'\2í•˜ì—¬ \1í•œë‹¤'),
                (r'(\w+)ì´ë©°, (\w+)ì´ë‹¤', r'\2ì´ê³ , \1ì´ë‹¤'),
                (r'(\w+) ê·¸ë¦¬ê³  (\w+)', r'\2 ë° \1'),
            ]
        }
        
        # í‘œí˜„ ë°©ì‹ ë³€ê²½
        self.expression_changes = {
            "formal_to_informal": {
                "ê²ƒì´ë‹¤": "ê±°ë‹¤",
                "í•˜ëŠ” ê²ƒ": "í•˜ê¸°",
                "ë˜ëŠ” ê²ƒ": "ë˜ê¸°",
                "ìˆëŠ” ê²ƒ": "ìˆê¸°"
            },
            "informal_to_formal": {
                "ê±°ë‹¤": "ê²ƒì´ë‹¤",
                "í•˜ê¸°": "í•˜ëŠ” ê²ƒ",
                "ë˜ê¸°": "ë˜ëŠ” ê²ƒ",
                "ìˆê¸°": "ìˆëŠ” ê²ƒ"
            },
            "academic_enhancement": {
                "ë§ë‹¤": "ë‹¤ìˆ˜ì´ë‹¤",
                "ì ë‹¤": "ì†Œìˆ˜ì´ë‹¤",
                "í¬ë‹¤": "ìƒë‹¹í•˜ë‹¤",
                "ì‘ë‹¤": "ë¯¸í¡í•˜ë‹¤",
                "ì¢‹ë‹¤": "ìš°ìˆ˜í•˜ë‹¤",
                "ë‚˜ì˜ë‹¤": "ë¶€ì ì ˆí•˜ë‹¤"
            }
        }
    
    def fix_plagiarized_text(self, original_text: str, plagiarism_matches: List[Dict]) -> List[PlagiarismFix]:
        """í‘œì ˆëœ í…ìŠ¤íŠ¸ë¥¼ AIê°€ ìë™ìœ¼ë¡œ ìˆ˜ì •"""
        print(f"ğŸ”§ AI í‘œì ˆ íšŒí”¼ ì‹œì‘: {len(plagiarism_matches)}ê°œ ë§¤ì¹˜ ê°ì§€")
        
        fixes = []
        
        # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_matches = sorted(plagiarism_matches, key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        for i, match in enumerate(sorted_matches):
            try:
                # ë§¤ì¹˜ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                start_idx = match.get('start_index', 0)
                end_idx = match.get('end_index', len(original_text))
                similarity = match.get('similarity_score', 0)
                
                # ìœ ì‚¬ë„ 90% ì´ìƒë§Œ ìˆ˜ì •
                if similarity < 0.9:
                    print(f"â­ï¸  ìœ ì‚¬ë„ {similarity:.1%} - ìˆ˜ì • ìŠ¤í‚µ")
                    continue
                
                matched_segment = original_text[start_idx:end_idx]
                print(f"ğŸ¯ ìˆ˜ì • ëŒ€ìƒ {i+1}: '{matched_segment[:50]}...' (ìœ ì‚¬ë„: {similarity:.1%})")
                
                # AI ê¸°ë°˜ ìˆ˜ì • ì ìš©
                fixed_segment = self._apply_ai_fixes(matched_segment, similarity)
                
                if fixed_segment and fixed_segment != matched_segment:
                    # ì˜ˆìƒ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ê³„ì‚°)
                    estimated_similarity = self._estimate_similarity_reduction(matched_segment, fixed_segment)
                    
                    fix = PlagiarismFix(
                        original_segment=matched_segment,
                        fixed_segment=fixed_segment,
                        similarity_before=similarity,
                        similarity_after=estimated_similarity,
                        fix_type="ai_automatic_fix",
                        confidence=0.85,
                        start_index=start_idx,
                        end_index=end_idx
                    )
                    
                    fixes.append(fix)
                    print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {similarity:.1%} â†’ {estimated_similarity:.1%}")
                else:
                    print(f"âŒ ìˆ˜ì • ì‹¤íŒ¨: ë³€ê²½ì‚¬í•­ ì—†ìŒ")
                    
            except Exception as e:
                print(f"âŒ ìˆ˜ì • ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ğŸ‰ AI í‘œì ˆ íšŒí”¼ ì™„ë£Œ: {len(fixes)}ê°œ ìˆ˜ì •")
        return fixes
    
    def _apply_ai_fixes(self, text: str, similarity: float) -> str:
        """AI ê¸°ë°˜ ë‹¤ì¸µ ìˆ˜ì • ì ìš©"""
        fixed_text = text
        
        # 1ë‹¨ê³„: ë™ì˜ì–´ êµì²´ (ê°€ì¥ íš¨ê³¼ì )
        fixed_text = self._apply_synonym_replacement(fixed_text)
        
        # 2ë‹¨ê³„: ë¬¸ì¥ êµ¬ì¡° ë³€ê²½
        if similarity > 0.95:  # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„
            fixed_text = self._apply_structure_changes(fixed_text)
        
        # 3ë‹¨ê³„: í‘œí˜„ ë°©ì‹ ë³€ê²½
        if similarity > 0.92:
            fixed_text = self._apply_expression_changes(fixed_text)
        
        # 4ë‹¨ê³„: ë¬¸ì¥ ìˆœì„œ ì¡°ì •
        if similarity > 0.90:
            fixed_text = self._apply_sentence_reordering(fixed_text)
        
        return fixed_text
    
    def _apply_synonym_replacement(self, text: str) -> str:
        """ë™ì˜ì–´ êµì²´ ì ìš©"""
        result = text
        
        for original, synonyms in self.plagiarism_synonyms.items():
            if original in result:
                # ê°€ì¥ ì ì ˆí•œ ë™ì˜ì–´ ì„ íƒ (ë¬¸ë§¥ ê³ ë ¤)
                best_synonym = self._select_best_synonym(original, synonyms, result)
                result = result.replace(original, best_synonym)
                print(f"  ğŸ”„ ë™ì˜ì–´ êµì²´: '{original}' â†’ '{best_synonym}'")
        
        return result
    
    def _select_best_synonym(self, original: str, synonyms: List[str], context: str) -> str:
        """ë¬¸ë§¥ì— ë§ëŠ” ìµœì ì˜ ë™ì˜ì–´ ì„ íƒ"""
        # ë¬¸ë§¥ ë¶„ì„ì„ í†µí•œ ë™ì˜ì–´ ì„ íƒ
        if "í•™ìˆ " in context or "ì—°êµ¬" in context:
            # í•™ìˆ ì  ë§¥ë½
            academic_synonyms = [s for s in synonyms if len(s) > 2]
            return random.choice(academic_synonyms) if academic_synonyms else synonyms[0]
        elif "ë¹„ì¦ˆë‹ˆìŠ¤" in context or "ê²½ì˜" in context:
            # ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½
            business_synonyms = [s for s in synonyms if "ê²½ì˜" in s or "ë¹„ì¦ˆë‹ˆìŠ¤" in s]
            return business_synonyms[0] if business_synonyms else synonyms[0]
        else:
            # ì¼ë°˜ì  ë§¥ë½
            return random.choice(synonyms)
    
    def _apply_structure_changes(self, text: str) -> str:
        """ë¬¸ì¥ êµ¬ì¡° ë³€ê²½ ì ìš©"""
        result = text
        
        for pattern_type, patterns in self.structure_patterns.items():
            for pattern, replacement in patterns:
                if re.search(pattern, result):
                    result = re.sub(pattern, replacement, result)
                    print(f"  ğŸ”„ êµ¬ì¡° ë³€ê²½ ({pattern_type}): ì ìš©ë¨")
                    break  # í•œ ë²ˆë§Œ ì ìš©
        
        return result
    
    def _apply_expression_changes(self, text: str) -> str:
        """í‘œí˜„ ë°©ì‹ ë³€ê²½ ì ìš©"""
        result = text
        
        # í•™ìˆ ì  í‘œí˜„ ê°•í™”
        for original, enhanced in self.expression_changes["academic_enhancement"].items():
            if original in result:
                result = result.replace(original, enhanced)
                print(f"  ğŸ“ í•™ìˆ ì  ê°•í™”: '{original}' â†’ '{enhanced}'")
        
        return result
    
    def _apply_sentence_reordering(self, text: str) -> str:
        """ë¬¸ì¥ ìˆœì„œ ì¡°ì •"""
        sentences = text.split('.')
        if len(sentences) > 2:
            # ê°„ë‹¨í•œ ë¬¸ì¥ ìˆœì„œ ë³€ê²½
            sentences[0], sentences[1] = sentences[1], sentences[0]
            result = '.'.join(sentences)
            print(f"  ğŸ”„ ë¬¸ì¥ ìˆœì„œ ë³€ê²½ ì ìš©")
            return result
        return text
    
    def _estimate_similarity_reduction(self, original: str, fixed: str) -> float:
        """ìˆ˜ì • í›„ ì˜ˆìƒ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”)
        original_words = set(original.split())
        fixed_words = set(fixed.split())
        
        intersection = len(original_words & fixed_words)
        union = len(original_words | fixed_words)
        
        if union == 0:
            return 0.0
        
        jaccard_similarity = intersection / union
        
        # êµ¬ì¡° ë³€ê²½ ë“±ì„ ê³ ë ¤í•˜ì—¬ ì¶”ê°€ ê°ì†Œ
        structure_reduction = 0.1 if len(original) != len(fixed) else 0.05
        
        return max(0.0, jaccard_similarity - structure_reduction)
    
    def apply_fixes_to_full_text(self, original_text: str, fixes: List[PlagiarismFix]) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ì— ìˆ˜ì •ì‚¬í•­ ì ìš©"""
        result_text = original_text
        
        # ì¸ë±ìŠ¤ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì ìš© (ë’¤ì—ì„œë¶€í„° ìˆ˜ì •)
        sorted_fixes = sorted(fixes, key=lambda x: x.start_index, reverse=True)
        
        for fix in sorted_fixes:
            # í…ìŠ¤íŠ¸ êµì²´
            result_text = (
                result_text[:fix.start_index] + 
                fix.fixed_segment + 
                result_text[fix.end_index:]
            )
            
            print(f"âœ… ì ìš©ë¨: {fix.similarity_before:.1%} â†’ {fix.similarity_after:.1%}")
        
        return result_text
    
    def generate_fix_report(self, fixes: List[PlagiarismFix]) -> Dict:
        """ìˆ˜ì • ë³´ê³ ì„œ ìƒì„±"""
        if not fixes:
            return {
                "total_fixes": 0,
                "average_similarity_reduction": 0,
                "fixes": []
            }
        
        total_reduction = sum(fix.similarity_before - fix.similarity_after for fix in fixes)
        average_reduction = total_reduction / len(fixes)
        
        return {
            "total_fixes": len(fixes),
            "average_similarity_reduction": average_reduction,
            "total_similarity_reduction": total_reduction,
            "fixes": [
                {
                    "original": fix.original_segment[:100] + "..." if len(fix.original_segment) > 100 else fix.original_segment,
                    "fixed": fix.fixed_segment[:100] + "..." if len(fix.fixed_segment) > 100 else fix.fixed_segment,
                    "similarity_before": f"{fix.similarity_before:.1%}",
                    "similarity_after": f"{fix.similarity_after:.1%}",
                    "reduction": f"{fix.similarity_before - fix.similarity_after:.1%}",
                    "fix_type": fix.fix_type,
                    "confidence": f"{fix.confidence:.1%}"
                }
                for fix in fixes
            ]
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    fixer = AIPlagiarismFixer()
    
    test_text = "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. ì—°êµ¬ ê²°ê³¼ì— ë”°ë¥´ë©´ ìƒˆë¡œìš´ ê¸°ìˆ ì´ ì œì‹œë˜ì—ˆë‹¤."
    test_matches = [
        {
            "start_index": 0,
            "end_index": 30,
            "similarity_score": 0.95
        }
    ]
    
    fixes = fixer.fix_plagiarized_text(test_text, test_matches)
    report = fixer.generate_fix_report(fixes)
    
    print("\nğŸ“Š ìˆ˜ì • ë³´ê³ ì„œ:")
    print(f"ì´ ìˆ˜ì •: {report['total_fixes']}ê°œ")
    print(f"í‰ê·  ìœ ì‚¬ë„ ê°ì†Œ: {report['average_similarity_reduction']:.1%}")