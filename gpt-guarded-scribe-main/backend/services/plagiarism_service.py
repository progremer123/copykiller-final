from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import List, Optional
import time
from datetime import datetime

# DocumentSource ëª¨ë¸ì„ import í•´ì•¼ í•©ë‹ˆë‹¤.
from models import PlagiarismCheck, PlagiarismMatch, DocumentSource
from services.text_processor import TextProcessor
from services.similarity_calculator import SimilarityCalculator
from services.web_crawler_service import WebCrawlerService
from services.ai_analysis_service import AIAnalysisService, PlagiarismContextAnalyzer
from services.realtime_improvement_service import RealTimeImprovementService

class PlagiarismService:
    def __init__(self, db: Session):
        self.db = db
        self.text_processor = TextProcessor()
        self.similarity_calculator = SimilarityCalculator()
        self.web_crawler = WebCrawlerService()
        self.ai_analysis = AIAnalysisService()
        self.context_analyzer = PlagiarismContextAnalyzer()
        self.improvement_service = RealTimeImprovementService()

    def create_check(self, check_id: str, text: str, file_name: str = None, file_type: str = None) -> PlagiarismCheck:
        """ìƒˆë¡œìš´ í‘œì ˆ ê²€ì‚¬ ìƒì„±"""
        check = PlagiarismCheck(
            id=check_id,
            original_text=text,
            file_name=file_name,
            file_type=file_type,
            status="checking"
        )
        self.db.add(check)
        self.db.commit()
        return check

    def process_plagiarism_check(self, check_id: str, text: str):
        """í‘œì ˆ ê²€ì‚¬ ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
        start_time = time.time()
        
        try:
            print(f"[*] í‘œì ˆ ê²€ì‚¬ ì‹œì‘: {check_id}")
            print(f"[*] ì…ë ¥ í…ìŠ¤íŠ¸: {text[:50]}...")
            
            # ê²€ì‚¬ ê°ì²´ê°€ ì—†ìœ¼ë©´ ìƒì„±
            existing_check = self.db.query(PlagiarismCheck).filter(PlagiarismCheck.id == check_id).first()
            if not existing_check:
                print(f"[NEW] ìƒˆ ê²€ì‚¬ ê°ì²´ ìƒì„±: {check_id}")
                check = PlagiarismCheck(
                    id=check_id,
                    original_text=text,
                    status="checking"
                )
                self.db.add(check)
                self.db.commit()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            source_count = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
            print(f"[DB] í™œì„± ë¬¸ì„œ ìˆ˜: {source_count}ê°œ")
            
            # ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ (ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ê°œì„ )
            if source_count < 50:  # ë¬¸ì„œê°€ ì ìœ¼ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§
                self._schedule_background_crawling(text)
            
            if source_count == 0:
                print("[!] ë°ì´í„°ë² ì´ìŠ¤ì— ë¹„êµí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤! ê¸°ë³¸ ë°ì´í„° ìƒì„±...")
                # ê¸°ë³¸ ë°ì´í„° ëª‡ ê°œ ìƒì„±
                self._create_sample_data()
                # source_count ë‹¤ì‹œ í™•ì¸
                source_count = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
                print(f"[DB] ê¸°ë³¸ ë°ì´í„° ìƒì„± í›„: {source_count}ê°œ")
            
            processed_text = self.text_processor.preprocess_text(text)
            n_grams = self.text_processor.generate_ngrams(processed_text, n=5)
            
            print(f"[*] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ, ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            
            matches = self._find_matches(text, processed_text, n_grams)
            
            overall_similarity = self._calculate_overall_similarity(matches)
            
            print(f"[OK] ê²€ì‚¬ ì™„ë£Œ: ìœ ì‚¬ë„ {overall_similarity:.1f}%, ë§¤ì¹˜ {len(matches)}ê°œ")
            
            self._save_results(check_id, matches, overall_similarity, time.time() - start_time)
            
        except Exception as e:
            print(f"[ERROR] í‘œì ˆ ê²€ì‚¬ ì˜¤ë¥˜ {check_id}: {str(e)}")
            import traceback
            traceback.print_exc()
            self._update_check_status(check_id, "error")
    
    def _crawl_additional_data(self, text: str):
        """í…ìŠ¤íŠ¸ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì›¹ í¬ë¡¤ë§"""
        print("[*] ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘...")
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        words = text.split()
        keywords = []
        
        # í•œê¸€ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ê¸¸ì´ 2ì ì´ìƒ)
        for word in words:
            clean_word = word.strip('.,!?()[]{}":;')
            if len(clean_word) >= 2 and any('\u3131' <= char <= '\u318E' or '\uAC00' <= char <= '\uD7A3' for char in clean_word):
                keywords.append(clean_word)
        
        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
        top_keywords = keywords[:3] if keywords else ["ì¼ë°˜", "ì •ë³´", "ë‚´ìš©"]
        
        for keyword in top_keywords:
            try:
                print(f"[*] í‚¤ì›Œë“œ '{keyword}'ë¡œ í¬ë¡¤ë§ ì¤‘...")
                result = self.web_crawler.crawl_and_save(keyword, 2)  # í‚¤ì›Œë“œë‹¹ 2ê°œ ë¬¸ì„œ
                print(f"[OK] '{keyword}' í¬ë¡¤ë§ ê²°ê³¼: {result['saved_count']}ê°œ ì €ì¥")
            except Exception as e:
                print(f"[ERROR] '{keyword}' í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    def _find_matches(self, original_text: str, processed_text: str, n_grams) -> List[dict]:
        """ìŠ¤ë§ˆíŠ¸ ìœ ì‚¬ë„ ê²€ì‚¬ - ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­"""
        matches = []
        all_sources = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).all()
        
        print(f"[DB] ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜: {len(all_sources)}ê°œ")
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”: ì—¬ëŸ¬ ê³µë°±, ì¤„ë°”ê¿ˆì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        normalized_original = ' '.join(original_text.split())
        normalized_original_lower = normalized_original.lower()
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (2ì ì´ìƒ, ìˆ«ì ì œì™¸)
        original_words = [w.lower() for w in normalized_original.split() 
                         if len(w) >= 2 and not w.isdigit()]
        original_word_set = set(original_words)
        
        print(f"[*] ì¶”ì¶œëœ ë‹¨ì–´ ìˆ˜: {len(original_word_set)}ê°œ (ì˜ˆ: {list(original_word_set)[:5]}...)")
        
        for source in all_sources:
            print(f"[*] '{source.title}' ê²€ì‚¬ ì¤‘...")
            
            # ì†ŒìŠ¤ í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_source = ' '.join(source.content.split()).lower()
            source_words = set(w.lower() for w in normalized_source.split() 
                              if len(w) >= 2 and not w.isdigit())
            
            # ê³µí†µ ë‹¨ì–´ ì°¾ê¸°
            common_words = original_word_set.intersection(source_words)
            
            print(f"   ê³µí†µ ë‹¨ì–´: {len(common_words)}ê°œ")
            
            if len(common_words) > 0:
                # ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)
                union_size = len(original_word_set.union(source_words))
                similarity = (len(common_words) / union_size * 100) if union_size > 0 else 0
                
                # ì¶”ê°€ ìœ ì‚¬ë„ ê³„ì‚°: ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨
                common_ratio = len(common_words) / len(original_word_set) * 100 if original_word_set else 0
                
                print(f"   ê³„ì‚°ëœ ìœ ì‚¬ë„: {similarity:.1f}% (ë¹„ìœ¨: {common_ratio:.1f}%)")
                
                # ìµœì†Œ ìœ ì‚¬ë„ 2% ì´ìƒì´ê±°ë‚˜ ê³µí†µ ë‹¨ì–´ 2ê°œ ì´ìƒì´ë©´ ë§¤ì¹˜ë¡œ ì¸ì •
                if similarity >= 2 or len(common_words) >= 2:
                    # ê³µí†µ ë‹¨ì–´ë¡œ ë§¤ì¹˜ ìƒì„±
                    matched_text = " ".join(sorted(list(common_words))[:15])  # ìƒìœ„ 15ê°œ ë‹¨ì–´
                    
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê³µí†µ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì°¾ê¸°
                    text_lower = original_text.lower()
                    first_match_pos = 0
                    for word in common_words:
                        pos = text_lower.find(word.lower())
                        if pos >= 0:
                            first_match_pos = pos
                            break
                    
                    # ìµœì¢… ìœ ì‚¬ë„: Jaccard ìœ ì‚¬ë„ + ê³µí†µ ë‹¨ì–´ ë³´ë„ˆìŠ¤
                    final_similarity = min(similarity + (len(common_words) * 2), 95)
                    
                    matches.append({
                        "matched_text": matched_text,
                        "source_title": source.title,
                        "source_url": source.url,
                        "similarity_score": final_similarity,
                        "start_index": first_match_pos,
                        "end_index": first_match_pos + len(matched_text),
                        "match_type": "keyword"
                    })
                    print(f"[OK] ë§¤ì¹˜ ë°œê²¬: {final_similarity:.1f}% - ê³µí†µë‹¨ì–´: {len(common_words)}ê°œ")
                else:
                    print(f"   ìœ ì‚¬ë„ ë‚®ìŒ (ì„ê³„ê°’ ë¯¸ë‹¬)")
            else:
                print(f"   ê³µí†µ ë‹¨ì–´ ì—†ìŒ")
        
        print(f"[RESULT] ì´ {len(matches)}ê°œì˜ ë§¤ì¹˜ ë°œê²¬")
        return matches

    def _find_matching_segments(self, original_text: str, source_content: str, similarity_score: float) -> List[dict]:
        """ë§¤ì¹˜ë˜ëŠ” í…ìŠ¤íŠ¸ êµ¬ê°„ ì°¾ê¸° - ê°„ì†Œí™”ëœ ë²„ì „"""
        return [{
            "text": original_text[:50] + "..." if len(original_text) > 50 else original_text,
            "start": 0,
            "end": min(50, len(original_text))
        }]


    def _calculate_overall_similarity(self, matches: List[dict]) -> float:
        """ì „ì²´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not matches:
            return 0.0
        
        # ë‹¨ìˆœí•˜ê²Œ: ë§¤ì¹˜ë“¤ì˜ ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· 
        avg_similarity = sum(match.get("similarity_score", 0) for match in matches) / len(matches)
        
        # ë§¤ì¹˜ ê°œìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ (ê°€ì¤‘ì¹˜)
        match_count_bonus = min(len(matches) * 2, 15)  # ìµœëŒ€ +15%
        
        # ìµœì¢… ìœ ì‚¬ë„
        overall_similarity = avg_similarity + match_count_bonus
        
        # ë²”ìœ„ ì œí•œ
        overall_similarity = max(0.0, min(overall_similarity, 95.0))
        
        print(f"[CALC] ìœ ì‚¬ë„: {avg_similarity:.1f}% (í‰ê· ) + {match_count_bonus:.1f}% (ë³´ë„ˆìŠ¤) = {overall_similarity:.1f}%")
        
        return overall_similarity

    def _save_results(self, check_id: str, matches: List[dict], similarity_score: float, processing_time: float):
        """ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        print(f"[SAVE] ê²°ê³¼ ì €ì¥ ì¤‘: check_id={check_id}, ìœ ì‚¬ë„={similarity_score}%, ë§¤ì¹˜={len(matches)}ê°œ")
        
        check = self.db.query(PlagiarismCheck).filter(PlagiarismCheck.id == check_id).first()
        if check:
            print(f"[OK] ê²€ì‚¬ ê°ì²´ ë°œê²¬: {check.id}")
            check.similarity_score = similarity_score
            check.status = "completed"
            check.processing_time = processing_time
            check.updated_at = datetime.utcnow()
            
            for i, match_data in enumerate(matches, 1):
                print(f"[*] ë§¤ì¹˜ {i} ì €ì¥ ì¤‘: {match_data['source_title'][:30]}...")
                match = PlagiarismMatch(
                    check_id=check_id,
                    matched_text=match_data["matched_text"],
                    source_text=match_data["matched_text"],
                    source_title=match_data["source_title"],
                    source_url=match_data["source_url"],
                    similarity_score=match_data["similarity_score"],
                    start_index=match_data["start_index"],
                    end_index=match_data["end_index"]
                )
                self.db.add(match)
            
            self.db.commit()
            print(f"[OK] ì €ì¥ ì™„ë£Œ!")
        else:
            print(f"[ERROR] ê²€ì‚¬ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {check_id}")
            # ìƒˆë¡œìš´ ê²€ì‚¬ ê°ì²´ ìƒì„±
            check = PlagiarismCheck(
                id=check_id,
                original_text="",  # ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš°
                similarity_score=similarity_score,
                status="completed",
                processing_time=processing_time,
                updated_at=datetime.utcnow()
            )
            self.db.add(check)
            
            for match_data in matches:
                match = PlagiarismMatch(
                    check_id=check_id,
                    matched_text=match_data["matched_text"],
                    source_text=match_data["matched_text"],
                    source_title=match_data["source_title"],
                    source_url=match_data["source_url"],
                    similarity_score=match_data["similarity_score"],
                    start_index=match_data["start_index"],
                    end_index=match_data["end_index"]
                )
                self.db.add(match)
            
            self.db.commit()
            print(f"[OK] ìƒˆ ê°ì²´ ìƒì„± í›„ ì €ì¥ ì™„ë£Œ!")

    def _update_check_status(self, check_id: str, status: str):
        """ê²€ì‚¬ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        check = self.db.query(PlagiarismCheck).filter(PlagiarismCheck.id == check_id).first()
        if check:
            check.status = status
            check.updated_at = datetime.utcnow()
            self.db.commit()

    def get_check_result(self, check_id: str) -> Optional[PlagiarismCheck]:
        """ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ"""
        return self.db.query(PlagiarismCheck).filter(PlagiarismCheck.id == check_id).first()

    def get_check_history(self, limit: int = 10, offset: int = 0) -> List[PlagiarismCheck]:
        """ê²€ì‚¬ ì´ë ¥ ì¡°íšŒ"""
        return (
            self.db.query(PlagiarismCheck)
            .order_by(desc(PlagiarismCheck.created_at))
            .offset(offset)
            .limit(limit)
            .all()
        )

    def delete_check(self, check_id: str) -> bool:
        """ê²€ì‚¬ ê²°ê³¼ ì‚­ì œ"""
        check = self.db.query(PlagiarismCheck).filter(PlagiarismCheck.id == check_id).first()
        if check:
            self.db.query(PlagiarismMatch).filter(PlagiarismMatch.check_id == check_id).delete()
            self.db.delete(check)
            self.db.commit()
            return True
        return False

    def _create_sample_data(self):
        """ê¸°ë³¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        sample_documents = [
            {
                "title": "ì¸ê³µì§€ëŠ¥ ê°œìš”",
                "content": "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ í†µí•´ ì»´í“¨í„°ê°€ í•™ìŠµí•˜ê³  íŒë‹¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬, ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "source_type": "academic",
                "url": "sample_ai_overview"
            },
            {
                "title": "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ",
                "content": "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìœ¼ë©°, ê°ê° ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                "source_type": "academic",
                "url": "sample_ml_basics"
            },
            {
                "title": "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ",
                "content": "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„, êµ¬ë¬¸ ë¶„ì„, ì˜ë¯¸ ë¶„ì„ ë“±ì˜ ë‹¨ê³„ë¥¼ ê±°ì³ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
                "source_type": "academic",
                "url": "sample_nlp"
            },
            {
                "title": "ë”¥ëŸ¬ë‹ ì‘ìš©",
                "content": "ë”¥ëŸ¬ë‹ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê²€ì¶œ, ì–¸ì–´ ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.",
                "source_type": "academic",
                "url": "sample_deep_learning"
            }
        ]
        
        for doc_data in sample_documents:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            existing = self.db.query(DocumentSource).filter(DocumentSource.url == doc_data["url"]).first()
            if not existing:
                doc = DocumentSource(
                    title=doc_data["title"],
                    content=doc_data["content"],
                    source_type=doc_data["source_type"],
                    url=doc_data["url"],
                    is_active=True
                )
                self.db.add(doc)
        
        self.db.commit()
        print("[OK] ê¸°ë³¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ")

    def _schedule_background_crawling(self, text: str):
        """ìŠ¤ë§ˆíŠ¸ ë°±ê·¸ë¼ìš´ë“œ ì›¹ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§"""
        import threading
        from datetime import datetime, timedelta
        
        # ìµœê·¼ 1ì‹œê°„ ë‚´ì— í¬ë¡¤ë§í–ˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
        cache_key = f"crawl_cache_{hash(text[:100])}"  # í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
        
        def background_crawl():
            try:
                print("[*] ìŠ¤ë§ˆíŠ¸ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œì‘...")
                
                # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
                current_count = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
                print(f"[*] í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_count}ê°œ ë¬¸ì„œ")
                
                if current_count < 100:  # 100ê°œ ë¯¸ë§Œì¼ ë•Œë§Œ í¬ë¡¤ë§
                    self._crawl_additional_data_optimized(text)
                    
                    # í¬ë¡¤ë§ í›„ ìƒíƒœ í™•ì¸
                    new_count = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
                    added = new_count - current_count
                    print(f"[OK] ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ: {added}ê°œ ì¶”ê°€ë¨ (ì´ {new_count}ê°œ)")
                else:
                    print("[*] ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì–´ í¬ë¡¤ë§ ìƒëµ")
                    
            except Exception as e:
                print(f"[ERROR] ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (ë©”ì¸ ì‘ë‹µì— ì˜í–¥ ì—†ìŒ)
        thread = threading.Thread(target=background_crawl, daemon=True)
        thread.start()
        print("[*] ìŠ¤ë§ˆíŠ¸ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë¨ (ì‘ë‹µ ì§€ì—° ì—†ìŒ)")

    def _crawl_additional_data_optimized(self, text: str):
        """ìµœì í™”ëœ ì›¹ í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œìš©)"""
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)
        import re
        from collections import Counter
        
        # í•œê¸€ ëª…ì‚¬ë§Œ ì¶”ì¶œ (2-5ê¸€ì)
        korean_words = re.findall(r'[ê°€-í£]{2,5}', text)
        
        # ë¶ˆìš©ì–´ ì œê±° (í™•ì¥ëœ ëª©ë¡)
        stop_words = {
            'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'í•˜ë‚˜', 'ë•Œë¬¸', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ìˆëŠ”', 'ì—†ëŠ”', 'ê°™ì€', 'ë‹¤ë¥¸',
            'ê²ƒì„', 'ê²ƒì´', 'ê²ƒì€', 'ì´ë¥¼', 'ì´ëŠ”', 'ê·¸ë¥¼', 'ê·¸ëŠ”', 'ì €ë¥¼', 'ì €ëŠ”', 'í• ìˆ˜', 'ìˆê²Œ', 'ë˜ëŠ”',
            'í•˜ëŠ”', 'ë˜ê³ ', 'ìˆë‹¤', 'ìˆê³ ', 'ì—†ê³ ', 'ê°™ì´', 'ì²˜ëŸ¼', 'ì •ë„', 'ë¶€ë¶„', 'ê²½ìš°', 'ë•Œì™€', 'ê²½ìš°',
            'ì‚¬ëŒ', 'ì—¬ì', 'ë‚¨ì', 'ì•„ì´', 'í•™ìƒ', 'ì„ ìƒ', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì €ë²ˆ', 'ì§€ë‚œ', 'ì˜¬í•´', 'ì‘ë…„'
        }
        korean_words = [word for word in korean_words if word not in stop_words and len(word) >= 2]
        
        # ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ ì„ íƒ (ì¡°ê±´ ì™„í™”)
        word_freq = Counter(korean_words)
        
        # ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ ìš°ì„ , ì—†ìœ¼ë©´ ê¸¸ì´ê°€ ê¸´ ë‹¨ì–´ ì„ íƒ
        frequent_words = [word for word, count in word_freq.most_common(5) if count >= 2]
        unique_words = [word for word, count in word_freq.most_common(10) if count == 1 and len(word) >= 3]
        
        # ìµœì¢… í‚¤ì›Œë“œ ì„ íƒ (ë¹ˆë„ ë†’ì€ ë‹¨ì–´ + ê¸´ ë‹¨ì–´)
        top_keywords = frequent_words[:2] + unique_words[:2]
        top_keywords = list(set(top_keywords))[:3]  # ì¤‘ë³µ ì œê±° ë° 3ê°œ ì œí•œ
        
        if not top_keywords:
            # ë§ˆì§€ë§‰ fallback: í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ê¸´ ë‹¨ì–´ë“¤ ì„ íƒ
            top_keywords = sorted(set(korean_words), key=len, reverse=True)[:3]
        
        if not top_keywords:
            top_keywords = ["ì •ë³´", "ê¸°ìˆ ", "ì‚¬íšŒ"]  # ë²”ìš©ì ì¸ ê¸°ë³¸ í‚¤ì›Œë“œ
        
        print(f"ğŸ” ì„ íƒëœ í‚¤ì›Œë“œ: {top_keywords}")
        print(f"ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:50]}...")
        
        for keyword in top_keywords:
            try:
                print(f"[*] '{keyword}' í¬ë¡¤ë§ ì¤‘...")
                result = self.web_crawler.crawl_and_save(keyword, 3)  # í‚¤ì›Œë“œë‹¹ 3ê°œ ë¬¸ì„œ
                print(f"[OK] '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ: {result.get('saved_count', 0)}ê°œ ì €ì¥")
                
                # ê° í‚¤ì›Œë“œë§ˆë‹¤ 1ì´ˆ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                import time
                time.sleep(1)
                
            except Exception as e:
                print(f"[ERROR] '{keyword}' í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue

    def get_database_stats(self) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
        try:
            total_docs = self.db.query(DocumentSource).filter(DocumentSource.is_active == True).count()
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            wikipedia_count = self.db.query(DocumentSource).filter(
                DocumentSource.is_active == True,
                DocumentSource.source_type == 'wikipedia'
            ).count()
            
            namuwiki_count = self.db.query(DocumentSource).filter(
                DocumentSource.is_active == True, 
                DocumentSource.source_type == 'namuwiki'
            ).count()
            
            academic_count = self.db.query(DocumentSource).filter(
                DocumentSource.is_active == True,
                DocumentSource.source_type == 'academic'
            ).count()
            
            return {
                "total_documents": total_docs,
                "sources": {
                    "wikipedia": wikipedia_count,
                    "namuwiki": namuwiki_count,
                    "academic": academic_count
                },
                "status": "healthy" if total_docs > 20 else "needs_more_data"
            }
        except Exception as e:
            return {
                "error": str(e),
                "status": "error"
            }

    def cleanup_old_data(self, days_old: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        # ì˜¤ë˜ëœ ê²€ì‚¬ ê²°ê³¼ ì‚­ì œ
        old_checks = self.db.query(PlagiarismCheck).filter(
            PlagiarismCheck.created_at < cutoff_date
        ).all()
        
        deleted_count = 0
        for check in old_checks:
            # ê´€ë ¨ ë§¤ì¹˜ ë¨¼ì € ì‚­ì œ
            self.db.query(PlagiarismMatch).filter(PlagiarismMatch.check_id == check.id).delete()
            self.db.delete(check)
            deleted_count += 1
        
        self.db.commit()
        print(f"[*] {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ê²€ì‚¬ ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ")
        
        return deleted_count

    def _check_sentence_similarity(self, original_sentences: List[str], source_content: str, source) -> List[dict]:
        """ë¬¸ì¥ ë‹¨ìœ„ ìœ ì‚¬ë„ ê²€ì‚¬"""
        matches = []
        source_sentences = [s.strip() for s in source_content.split('.') if s.strip()]
        original_full_text = '. '.join(original_sentences)  # ì›ë³¸ ì „ì²´ í…ìŠ¤íŠ¸ ë³µì›
        
        for orig_sentence in original_sentences:
            if len(orig_sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                continue
                
            orig_words = set(orig_sentence.lower().split())
            
            for src_sentence in source_sentences:
                if len(src_sentence) < 10:
                    continue
                    
                src_words = set(src_sentence.lower().split())
                common_words = orig_words.intersection(src_words)
                
                # ê³µí†µ ë‹¨ì–´ê°€ 2ê°œ ì´ìƒì´ê³ , ì›ë¬¸ì˜ 30% ì´ìƒì¼ ë•Œ (ë” ê´€ëŒ€í•œ ì¡°ê±´)
                if len(common_words) >= 2 and len(common_words) / len(orig_words) >= 0.3:
                    # ë” ë³´ìˆ˜ì ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    word_ratio = len(common_words) / len(orig_words)
                    similarity = (word_ratio * 70) + (len(common_words) * 2)  # ìµœëŒ€ 80ì  ì •ë„
                    similarity = min(similarity, 80)  # ë¬¸ì¥ ë§¤ì¹­ ìµœëŒ€ 80%
                    
                    # ì›ë³¸ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì´ ë¬¸ì¥ì˜ ìœ„ì¹˜ ì°¾ê¸°
                    start_pos = original_full_text.lower().find(orig_sentence.lower())
                    if start_pos >= 0:
                        matches.append({
                            "matched_text": orig_sentence,
                            "source_title": source.title,
                            "source_url": source.url,
                            "similarity_score": similarity,
                            "start_index": start_pos,
                            "end_index": start_pos + len(orig_sentence),
                            "match_type": "sentence"
                        })
                        print(f"[*] ë¬¸ì¥ ë§¤ì¹˜: {similarity:.1f}% - {orig_sentence[:50]}...")
                        break
        
        return matches

    def _check_phrase_similarity(self, original_text: str, source_content: str, source) -> List[dict]:
        """êµ¬ë¬¸ ë‹¨ìœ„ ìœ ì‚¬ë„ ê²€ì‚¬ (3-7 ë‹¨ì–´)"""
        matches = []
        words = original_text.split()
        
        # 2-7ë‹¨ì–´ êµ¬ë¬¸ ìƒì„± (ë” ë§ì€ ë§¤ì¹˜ ì°¾ê¸°)
        for length in range(2, 8):
            for i in range(len(words) - length + 1):
                phrase = ' '.join(words[i:i+length])
                
                # ì†ŒìŠ¤ì—ì„œ ìœ ì‚¬í•œ êµ¬ë¬¸ ì°¾ê¸°
                if phrase.lower() in source_content.lower():
                    start_pos = original_text.lower().find(phrase.lower())
                    if start_pos >= 0:
                        # ë” ë³´ìˆ˜ì ì¸ êµ¬ë¬¸ ì ìˆ˜ ê³„ì‚°
                        base_score = 30 + (length * 5)  # 2ë‹¨ì–´=40ì , 3ë‹¨ì–´=45ì , 7ë‹¨ì–´=65ì 
                        phrase_score = min(base_score, 75)  # ìµœëŒ€ 75%
                        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ ì°¾ê¸°
                        actual_start = original_text.find(phrase)
                        if actual_start == -1:
                            # ì •í™•í•œ ë§¤ì¹˜ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê·¼ì‚¬ì¹˜ ì‚¬ìš©
                            actual_start = start_pos
                            actual_end = start_pos + len(phrase)
                        else:
                            actual_end = actual_start + len(phrase)
                        
                        matches.append({
                            "matched_text": phrase,
                            "source_title": source.title,
                            "source_url": source.url,
                            "similarity_score": phrase_score,
                            "start_index": actual_start,
                            "end_index": actual_end,
                            "match_type": "phrase"
                        })
                        print(f"[*] êµ¬ë¬¸ ë§¤ì¹˜: {85 + (length * 2)}% - {phrase[:40]}...")
        
        return matches

    def _check_keyword_similarity(self, original_words: List[str], source_content: str, source) -> List[dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬"""
        matches = []
        original_text = ' '.join(original_words)  # ì›ë³¸ í…ìŠ¤íŠ¸ ë³µì›
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ í•œê¸€, 3ê¸€ì ì´ìƒ ì˜ë¬¸)
        meaningful_words = []
        for word in original_words:
            clean_word = word.strip('.,!?()[]{}":;')
            if len(clean_word) >= 2:
                # í•œê¸€ ë˜ëŠ” ì˜ë¬¸ ì²´í¬
                if any('\uAC00' <= char <= '\uD7A3' for char in clean_word) or \
                   (clean_word.isalpha() and len(clean_word) >= 3):
                    meaningful_words.append(clean_word)
        
        if not meaningful_words:
            return matches
        
        # ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹˜ í™•ì¸
        source_lower = source_content.lower()
        matched_keywords = []
        keyword_positions = []
        
        for word in meaningful_words:
            if word.lower() in source_lower:
                matched_keywords.append(word)
                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì´ í‚¤ì›Œë“œì˜ ìœ„ì¹˜ ì°¾ê¸°
                pos = original_text.lower().find(word.lower())
                if pos >= 0:
                    keyword_positions.append({
                        "word": word,
                        "start": pos,
                        "end": pos + len(word)
                    })
        
        # ë§¤ì¹˜ëœ í‚¤ì›Œë“œê°€ ì¶©ë¶„íˆ ë§ìœ¼ë©´ ìœ ì‚¬ íŒì •
        match_ratio = len(matched_keywords) / len(meaningful_words)
        
        if match_ratio >= 0.3 and len(matched_keywords) >= 2:  # 30% ì´ìƒ, ìµœì†Œ 2ê°œ
            # í‚¤ì›Œë“œ ë§¤ì¹­ì€ ë‚®ì€ ì ìˆ˜
            similarity = min(match_ratio * 40, 50)  # ìµœëŒ€ 50%
            
            # ê° ë§¤ì¹˜ëœ í‚¤ì›Œë“œì— ëŒ€í•´ ê°œë³„ ë§¤ì¹˜ ìƒì„±
            for kw_pos in keyword_positions[:5]:  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ
                matches.append({
                    "matched_text": kw_pos["word"],
                    "source_title": source.title,
                    "source_url": source.url,
                    "similarity_score": similarity,
                    "start_index": kw_pos["start"],
                    "end_index": kw_pos["end"],
                    "match_type": "keyword"
                })
            
            print(f"[*] í‚¤ì›Œë“œ ë§¤ì¹˜: {similarity:.1f}% - {len(matched_keywords)}ê°œ í‚¤ì›Œë“œ")
        seen = set()
        
        for match in sorted_matches:
            key = f"{match['source_title']}_{match['start_index']//20}"  # 20ê¸€ì ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘ (ëœ ì—„ê²©)
            if key not in seen:
                unique_matches.append(match)
                seen.add(key)
        
        # ìƒìœ„ 20ê°œê¹Œì§€ ë°˜í™˜ (ë” ë§ì€ ë§¤ì¹˜ í‘œì‹œ)
        return unique_matches[:20]