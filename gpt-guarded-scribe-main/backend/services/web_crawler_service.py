#!/usr/bin/env python3
"""ì›¹ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""

import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import re
import time
from urllib.parse import urljoin, urlparse
from typing import List, Dict

class WebCrawlerService:
    def __init__(self, db_path="plagiarism.db"):
        self.db_path = db_path
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def crawl_search_results(self, query: str, num_results: int = 5) -> List[Dict]:
        """Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë¡¤ë§ (ì‹œë®¬ë ˆì´ì…˜)"""
        print(f"ğŸ” '{query}' ê²€ìƒ‰ ì¤‘...")
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Google Search APIë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ ì—”ì§„ API ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìš©ìœ¼ë¡œ ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ë“¤ì„ í¬ë¡¤ë§
        
        # ë‹¤ì–‘í•œ í•œêµ­ì–´ ì½˜í…ì¸  ì‚¬ì´íŠ¸ë“¤
        sample_urls = [
            "https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5",
            "https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5",
            "https://ko.wikipedia.org/wiki/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5",
            "https://ko.wikipedia.org/wiki/%EA%B8%B0%ED%9B%84_%EB%B3%80%ED%99%94",
            "https://ko.wikipedia.org/wiki/%EA%B5%90%EC%9C%A1",
            # ì¶”ê°€ í•œêµ­ì–´ ì½˜í…ì¸  ì‚¬ì´íŠ¸ë“¤
            "https://terms.naver.com/entry.naver?docId=3478014&cid=58439&categoryId=58439",  # ë„¤ì´ë²„ ì§€ì‹ë°±ê³¼
            "https://100.daum.net/encyclopedia/view/14XXE0031576",  # ë‹¤ìŒë°±ê³¼ (ì˜ˆì‹œ)
            "https://ko.wikipedia.org/wiki/%EC%A0%95%EC%B9%98",
            "https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%A0%9C",
            "https://ko.wikipedia.org/wiki/%EC%82%AC%ED%9A%8C",
        ]
        
        results = []
        for i, url in enumerate(sample_urls[:num_results]):
            try:
                content = self.crawl_article(url)
                if content and len(content.get('content', '')) > 100:
                    results.append(content)
                    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ {i+1}/{num_results}: {content['title'][:50]}...")
                else:
                    print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
                    
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {url} - {e}")
                
        return results
    
    def crawl_article(self, url: str) -> Dict:
        """ê°œë³„ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup) or self._extract_title_from_url(url)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content(soup)
            
            if content and len(content) > 100:
                return {
                    'title': title,
                    'content': self._clean_text(content),
                    'url': url,
                    'source_type': 'crawled'
                }
                
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            
        return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì œëª© ì„ íƒì ì‹œë„
        selectors = [
            'title',
            'h1',
            '.title',
            '#title',
            '[class*="title"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()[:200]
                
        return "ì œëª© ì—†ìŒ"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # ë³¸ë¬¸ í›„ë³´ë“¤
        content_selectors = [
            '.content',
            '.article',
            '.post',
            '#content',
            'main',
            '.main-content',
            'article',
            '.entry-content'
        ]
        
        # ì„ íƒìë¡œ ë³¸ë¬¸ ì°¾ê¸°
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = ' '.join([el.get_text() for el in elements])
                if len(content) > 200:
                    return content
        
        # ëª¨ë“  p íƒœê·¸ ë‚´ìš©
        paragraphs = soup.find_all('p')
        if paragraphs:
            content = ' '.join([p.get_text() for p in paragraphs])
            if len(content) > 200:
                return content
        
        # ì „ì²´ í…ìŠ¤íŠ¸
        return soup.get_text()
    
    def _extract_title_from_url(self, url: str) -> str:
        """URLì—ì„œ ì œëª© ì¶”ì¶œ"""
        parsed = urlparse(url)
        domain = parsed.netloc
        path = parsed.path
        
        return f"{domain}{path}"[:100]
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£.,!?]', '', text)
        # ê¸¸ì´ ì œí•œ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ìš©ëŸ‰ ê³ ë ¤)
        return text.strip()[:5000]
    
    def save_to_database(self, articles: List[Dict]) -> int:
        """í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not articles:
            return 0
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
            saved_count = 0
            
            for article in articles:
                # ì¤‘ë³µ ì²´í¬
                cursor.execute(
                    "SELECT id FROM document_sources WHERE url = ? AND is_active = 1",
                    (article['url'],)
                )
                
                if cursor.fetchone():
                    print(f"âš ï¸  ì´ë¯¸ ì¡´ì¬: {article['title'][:50]}...")
                    continue
                
                # ì €ì¥
                cursor.execute("""
                    INSERT INTO document_sources 
                    (title, content, url, source_type, created_at, updated_at, is_active)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['title'],
                    article['content'],
                    article['url'],
                    article['source_type'],
                    current_time,
                    current_time,
                    1
                ))
                saved_count += 1
                print(f"ğŸ’¾ ì €ì¥ë¨: {article['title'][:50]}...")
            
            conn.commit()
            conn.close()
            
            return saved_count
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return 0
    
    def crawl_and_save(self, query: str, num_results: int = 5) -> Dict:
        """ê²€ìƒ‰í•˜ê³  í¬ë¡¤ë§í•´ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        print(f"ğŸš€ ì›¹ í¬ë¡¤ë§ ì‹œì‘: '{query}'")
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§
        articles = self.crawl_search_results(query, num_results)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_count = self.save_to_database(articles)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'query': query,
            'total_crawled': len(articles),
            'saved_count': saved_count,
            'articles': [
                {
                    'title': article['title'][:100],
                    'content_length': len(article['content']),
                    'url': article['url']
                }
                for article in articles
            ]
        }
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ìˆ˜ì§‘, {saved_count}ê°œ ì €ì¥")
        return result

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    crawler = WebCrawlerService()
    
    # ì—¬ëŸ¬ ì£¼ì œë¡œ í¬ë¡¤ë§
    queries = ["ì¸ê³µì§€ëŠ¥", "ê¸°í›„ë³€í™”", "êµìœ¡", "ê±´ê°•"]
    
    for query in queries:
        result = crawler.crawl_and_save(query, 3)
        print(f"\nğŸ“Š ê²°ê³¼: {result}")
        print("-" * 50)