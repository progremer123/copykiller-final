#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import openai
import os
from typing import Dict, List

class AIAnalysisService:
    """AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.openai_api_key = os.getenv('OPENAI_API_KEY', 'your-api-key-here')
    
    def analyze_writing_style(self, text: str) -> Dict:
        """ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„ì„"""
        
        # ê°„ë‹¨í•œ í†µê³„ ë¶„ì„
        sentences = text.split('.')
        words = text.split()
        
        analysis = {
            "sentence_count": len([s for s in sentences if s.strip()]),
            "word_count": len(words),
            "avg_sentence_length": len(words) / max(len(sentences), 1),
            "complexity_score": 7.5,  # ì„ì‹œ ê°’
            "tone": self._analyze_tone(text),
            "detected_style": self._analyze_writing_style(text),
            "academic_score": self._calculate_academic_score(text),
            "improvement_areas": [
                "ë¬¸ì¥ ê¸¸ì´ ì¡°ì ˆ",
                "ì–´íœ˜ ë‹¤ì–‘ì„± ì¦ëŒ€",
                "ë…¼ë¦¬ì  ì—°ê²°ì„± ê°•í™”"
            ]
        }
        
        return analysis
    
    def _calculate_complexity(self, text: str) -> str:
        """ë¬¸ì¥ ë³µì¡ë„ ê³„ì‚°"""
        avg_word_length = sum(len(word) for word in text.split()) / max(len(text.split()), 1)
        
        if avg_word_length > 4:
            return "ë†’ìŒ"
        elif avg_word_length > 3:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def _analyze_tone(self, text: str) -> str:
        """ì–´ì¡° ë¶„ì„"""
        formal_indicators = ["ìŠµë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ê²ƒì…ë‹ˆë‹¤"]
        informal_indicators = ["í•´ìš”", "ì´ì—ìš”", "ê±°ì˜ˆìš”"]
        
        formal_count = sum(1 for indicator in formal_indicators if indicator in text)
        informal_count = sum(1 for indicator in informal_indicators if indicator in text)
        
        if formal_count > informal_count:
            return "ê²©ì‹ì²´"
        elif informal_count > formal_count:
            return "ë¹„ê²©ì‹ì²´"
        else:
            return "ì¤‘ë¦½"
    
    def _analyze_writing_style(self, text: str) -> str:
        """ë¬¸ì²´ ë¶„ì„"""
        academic_words = ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ì´ì— ë”°ë¼", "ê²°ê³¼ì ìœ¼ë¡œ", "ì—°êµ¬", "ë¶„ì„"]
        news_words = ["ë°œí‘œí–ˆë‹¤", "ë°í˜”ë‹¤", "ì „í–ˆë‹¤", "ë³´ë„ëë‹¤"]
        essay_words = ["ìƒê°í•œë‹¤", "ëŠë‚€ë‹¤", "ê°œì¸ì ìœ¼ë¡œ", "ë‚´ ì˜ê²¬ìœ¼ë¡œëŠ”"]
        
        academic_score = sum(1 for word in academic_words if word in text)
        news_score = sum(1 for word in news_words if word in text)
        essay_score = sum(1 for word in essay_words if word in text)
        
        scores = {"í•™ìˆ ë…¼ë¬¸": academic_score, "ë‰´ìŠ¤ê¸°ì‚¬": news_score, "ì—ì„¸ì´": essay_score}
        
        return max(scores, key=scores.get)
    
    def _calculate_academic_score(self, text: str) -> int:
        """í•™ìˆ ì„± ì ìˆ˜ ê³„ì‚° (1-100ì )"""
        academic_indicators = [
            "ì—°êµ¬", "ë¶„ì„", "ê²€í† ", "ê³ ì°°", "ë…¼ì˜", "ê²°ë¡ ", "ê°€ì„¤", "ì‹¤í—˜",
            "ë°ì´í„°", "ê²°ê³¼", "ë°©ë²•ë¡ ", "ì´ë¡ ", "ëª¨ë¸", "í”„ë ˆì„ì›Œí¬"
        ]
        
        score = 0
        for indicator in academic_indicators:
            if indicator in text:
                score += 5
        
        return min(score, 100)

class PlagiarismContextAnalyzer:
    """í‘œì ˆ ë§¥ë½ ë¶„ì„ê¸°"""
    
    def analyze_plagiarism_context(self, original_text: str, matches: List[Dict]) -> Dict:
        """í‘œì ˆ ë§¥ë½ ë¶„ì„"""
        
        risk_level = self._calculate_risk_level(matches)
        plagiarism_type = self._identify_plagiarism_type(matches)
        
        analysis = {
            "risk_score": self._calculate_risk_score(matches),
            "risk_level": risk_level,
            "plagiarism_types": [plagiarism_type],
            "legal_assessment": self._assess_legal_risk(matches),
            "severity": self._assess_severity(matches),
            "improvement_suggestions": self._generate_improvement_suggestions(matches)
        }
        
        return analysis
    
    def _calculate_risk_level(self, matches: List[Dict]) -> str:
        """ìœ„í—˜ë„ ê³„ì‚°"""
        if not matches:
            return "ì•ˆì „"
        
        max_similarity = max(match.get('similarity_score', 0) for match in matches)
        
        if max_similarity >= 80:
            return "ë§¤ìš° ìœ„í—˜"
        elif max_similarity >= 60:
            return "ìœ„í—˜"
        elif max_similarity >= 40:
            return "ì£¼ì˜"
        else:
            return "ë‚®ìŒ"
    
    def _identify_plagiarism_type(self, matches: List[Dict]) -> str:
        """í‘œì ˆ ìœ í˜• ë¶„ì„"""
        if not matches:
            return "í‘œì ˆ ì—†ìŒ"
        
        total_matches = len(matches)
        avg_similarity = sum(match.get('similarity_score', 0) for match in matches) / total_matches
        
        if avg_similarity > 70 and total_matches > 10:
            return "ì§ì ‘ ë³µì‚¬"
        elif avg_similarity > 50:
            return "ë¶€ë¶„ í‘œì ˆ"
        elif total_matches > 15:
            return "ëª¨ìì´í¬ í‘œì ˆ"
        else:
            return "ìœ ì‚¬ í‘œí˜„"
    
    def _generate_improvement_suggestions(self, matches: List[Dict]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        if not matches:
            return ["âœ… ë…ì°½ì ì¸ ë‚´ìš©ì…ë‹ˆë‹¤."]
        
        suggestions = []
        max_similarity = max(match.get('similarity_score', 0) for match in matches)
        
        if max_similarity > 70:
            suggestions.extend([
                "ğŸ”´ ë†’ì€ ìœ ì‚¬ë„ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "ğŸ’¡ ì¸ìš©ë¬¸ì„ ì‚¬ìš©í•  ê²½ìš° ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ì„¸ìš”.",
                "âœï¸ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”."
            ])
        elif max_similarity > 40:
            suggestions.extend([
                "ğŸŸ¡ ë¶€ë¶„ì  ìœ ì‚¬ì„±ì´ ìˆìŠµë‹ˆë‹¤.",
                "ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ë‹¤ì–‘í™”í•˜ì„¸ìš”.",
                "ğŸ“š ì¶”ê°€ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ë‚´ìš©ì„ ë³´ì™„í•´ë³´ì„¸ìš”."
            ])
        else:
            suggestions.append("ğŸŸ¢ ì ì ˆí•œ ìˆ˜ì¤€ì˜ ë…ì°½ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        
        return suggestions
    
    def _assess_severity(self, matches: List[Dict]) -> str:
        """ì‹¬ê°ë„ í‰ê°€"""
        if not matches:
            return "ë¬¸ì œì—†ìŒ"
        
        high_similarity_count = sum(1 for match in matches if match.get('similarity_score', 0) > 60)
        
        if high_similarity_count >= 5:
            return "ì‹¬ê°"
        elif high_similarity_count >= 2:
            return "ë³´í†µ"
        else:
            return "ê²½ë¯¸"
    
    def _assess_legal_risk(self, matches: List[Dict]) -> str:
        """ë²•ì  ìœ„í—˜ë„ í‰ê°€"""
        if not matches:
            return "ìœ„í—˜ì—†ìŒ"
        
        max_similarity = max(match.get('similarity_score', 0) for match in matches)
        
        if max_similarity >= 85:
            return "ë†’ìŒ - ì €ì‘ê¶Œ ì¹¨í•´ ê°€ëŠ¥ì„±"
        elif max_similarity >= 65:
            return "ë³´í†µ - ì£¼ì˜ í•„ìš”"
        else:
            return "ë‚®ìŒ"
    
    def _calculate_risk_score(self, matches: List[Dict]) -> float:
        """ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° (0-10ì )"""
        if not matches:
            return 0.0
        
        max_similarity = max(match.get('similarity_score', 0) for match in matches)
        match_count = len(matches)
        
        # ê¸°ë³¸ ì ìˆ˜ëŠ” ìµœëŒ€ ìœ ì‚¬ë„ ê¸°ë°˜
        base_score = max_similarity / 10
        
        # ë§¤ì¹˜ ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        count_weight = min(match_count * 0.5, 3.0)
        
        # ìµœì¢… ì ìˆ˜ (0-10ì )
        final_score = min(base_score + count_weight, 10.0)
        
        return round(final_score, 1)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # AI ë¶„ì„ í…ŒìŠ¤íŠ¸
    ai_service = AIAnalysisService()
    
    sample_text = """
    ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
    ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë§ì€ ë¶„ì•¼ì—ì„œ í˜ì‹ ì´ ì¼ì–´ë‚˜ê³  ìˆìœ¼ë©°, 
    íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë†€ë¼ìš´ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
    """
    
    style_analysis = ai_service.analyze_writing_style(sample_text)
    print("ğŸ“Š ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„ì„:")
    for key, value in style_analysis.items():
        print(f"   {key}: {value}")
    
    # í‘œì ˆ ë§¥ë½ ë¶„ì„ í…ŒìŠ¤íŠ¸
    context_analyzer = PlagiarismContextAnalyzer()
    
    sample_matches = [
        {"similarity_score": 75.5, "source_title": "AI ë…¼ë¬¸"},
        {"similarity_score": 45.2, "source_title": "ê¸°ìˆ  ë¸”ë¡œê·¸"}
    ]
    
    context_analysis = context_analyzer.analyze_plagiarism_context(sample_text, sample_matches)
    print(f"\nğŸ¯ í‘œì ˆ ë§¥ë½ ë¶„ì„:")
    for key, value in context_analysis.items():
        print(f"   {key}: {value}")